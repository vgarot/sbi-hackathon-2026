{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00463845",
   "metadata": {},
   "source": [
    "# Load simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330adeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "from teddy.data.Alphabet import Alphabet\n",
    "from teddy.data.dataset import MsaLabels\n",
    "\n",
    "lower_bound = torch.as_tensor([1.0, 1.0])\n",
    "upper_bound = torch.as_tensor([5.0, 10.0])\n",
    "prior = BoxUniform(low=lower_bound, high=upper_bound)\n",
    "\n",
    "alphabet = Alphabet([\"A\", \"C\", \"G\", \"T\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(clock_rate):\n",
    "    simulation = MsaLabels(f\"data/observation/seq\", alphabet)\n",
    "\n",
    "    theta_0 = torch.Tensor(simulation[0][1])\n",
    "    x_0 = torch.flatten(torch.Tensor(simulation[0][0][0]))\n",
    "\n",
    "    return theta_0, x_0\n",
    "\n",
    "def load_data(clock_rate):\n",
    "    simulations = MsaLabels(f\"data/clock_rate-{clock_rate}/seq\", alphabet)\n",
    "    \n",
    "    theta, x = [], []\n",
    "\n",
    "    for index, simulation in enumerate(simulations):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        theta.append(simulation[1])\n",
    "        x.append(simulation[0][0])\n",
    "\n",
    "    theta = torch.Tensor(np.array(theta))\n",
    "    x = torch.flatten(torch.Tensor(np.array(x)), start_dim=1)\n",
    "\n",
    "    print(f\"Data for clock rate 1e-{clock_rate} loaded.\")\n",
    "\n",
    "    return theta, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ff586",
   "metadata": {},
   "source": [
    "# NPE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "616cfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d3667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_npe(clock_rate, embedding=False):\n",
    "    if embedding:\n",
    "        embedding_net = FCEmbedding(\n",
    "            input_dim=x.shape[1],\n",
    "            output_dim=10,\n",
    "            num_layers=2,\n",
    "            num_hiddens=50\n",
    "        )\n",
    "        density_estimator = posterior_nn(model=\"maf\", embedding_net=embedding_net)\n",
    "        npe = NPE(prior=prior, density_estimator=density_estimator)\n",
    "    else:\n",
    "        npe = NPE(prior=prior)\n",
    "    \n",
    "    theta, x = load_data(clock_rate)\n",
    "    npe.append_simulations(theta, x)\n",
    "    npe.train()\n",
    "\n",
    "    print(f\"NPE for clock rate 1e-{clock_rate} trained.\")\n",
    "\n",
    "    posterior = npe.build_posterior()\n",
    "    return posterior\n",
    "\n",
    "def sample_npe(clock_rate, sample_size=10_000, embedding=False):\n",
    "    _, x_0 = observe(clock_rate)\n",
    "    posterior = train_npe(clock_rate, embedding)\n",
    "    samples = posterior.sample((sample_size,), x=x_0)\n",
    "    print(f\"Posterior for clock rate 1e-{clock_rate} sampled.\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4b0ac",
   "metadata": {},
   "source": [
    "# NPE plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de60e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sbi.analysis import pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6e1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_npe(clock_rate, sample_size=10_000, embedding=False):\n",
    "    theta_0, _ = observe(clock_rate)\n",
    "    samples = sample_npe(clock_rate, sample_size, embedding)\n",
    "\n",
    "    fig, axes = pairplot(\n",
    "        samples,\n",
    "        labels=[\"Basic reproduction number\", \"Infectious time\"],\n",
    "        figsize=(8, 8),\n",
    "        points=theta_0,\n",
    "        points_colors=\"r\",\n",
    "    )\n",
    "    plt.suptitle(f\"NPE Posterior (clock rate: 1e-{clock_rate})\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def compare_npe(clock_rates, sample_size=10_000, embedding=False):\n",
    "    samples, labels = [], []\n",
    "    for clock_rate in clock_rates:\n",
    "        samples.append(sample_npe(clock_rate, sample_size, embedding))\n",
    "        labels.append(f\"1e-{clock_rate}\")\n",
    "    \n",
    "    fig, axes = pairplot(\n",
    "        samples,\n",
    "        labels=[\"Basic reproduction number\", \"Infectious time\"],\n",
    "        figsize=(8, 8),\n",
    "        points_colors=\"r\",\n",
    "        diag=\"hist\",\n",
    "        upper=\"scatter\",\n",
    "    )\n",
    "    fig.legend(\n",
    "        labels,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(0.95, 0.95),\n",
    "    )\n",
    "    plt.suptitle(\"NPE Posterior Comparison (various clock rates)\", y=1.02, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9293b7",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5391047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation for clock rate 1e-0 loaded.\n",
      "Data for clock rate 1e-0 loaded.\n",
      " Neural network successfully converged after 32 epochs.NPE for clock rate 1e-0 trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b1b9e201134175b122ea9b2070223e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior for clock rate 1e-0 sampled.\n",
      "Observation for clock rate 1e-1 loaded.\n",
      "Data for clock rate 1e-1 loaded.\n",
      " Neural network successfully converged after 29 epochs.NPE for clock rate 1e-1 trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3de8d801584521bdab1f8444e89c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior for clock rate 1e-1 sampled.\n",
      "Observation for clock rate 1e-2 loaded.\n",
      "Data for clock rate 1e-2 loaded.\n",
      " Neural network successfully converged after 30 epochs.NPE for clock rate 1e-2 trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa4e20654ce4bf8b278831b3f4a81a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior for clock rate 1e-2 sampled.\n",
      "Observation for clock rate 1e-3 loaded.\n",
      "Data for clock rate 1e-3 loaded.\n",
      " Neural network successfully converged after 27 epochs.NPE for clock rate 1e-3 trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3a94ffca41465591ed4edb899d7824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcompare_npe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcompare_npe\u001b[39m\u001b[34m(clock_rates, sample_size, embedding)\u001b[39m\n\u001b[32m     16\u001b[39m samples, labels = [], []\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m clock_rate \u001b[38;5;129;01min\u001b[39;00m clock_rates:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     samples.append(\u001b[43msample_npe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclock_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m     labels.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m1e-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclock_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m fig, axes = pairplot(\n\u001b[32m     22\u001b[39m     samples,\n\u001b[32m     23\u001b[39m     labels=[\u001b[33m\"\u001b[39m\u001b[33mBasic reproduction number\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mInfectious time\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     upper=\u001b[33m\"\u001b[39m\u001b[33mscatter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36msample_npe\u001b[39m\u001b[34m(clock_rate, sample_size, embedding)\u001b[39m\n\u001b[32m     24\u001b[39m _, x_0 = observe(clock_rate)\n\u001b[32m     25\u001b[39m posterior = train_npe(clock_rate, embedding)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m samples = \u001b[43mposterior\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPosterior for clock rate 1e-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclock_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sampled.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/sbi/inference/posteriors/direct_posterior.py:180\u001b[39m, in \u001b[36mDirectPosterior.sample\u001b[39m\u001b[34m(self, sample_shape, x, max_sampling_batch_size, sample_with, show_progress_bars)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_with \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    175\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou set `sample_with=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_with\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. As of sbi v0.18.0, setting \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`sample_with` is no longer supported. You have to rerun \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`.build_posterior(sample_with=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_with\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m samples = \u001b[43mrejection\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_reject_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposterior_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_reject_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwithin_support\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sampling_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_sampling_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproposal_sampling_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcondition\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43malternative_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbuild_posterior(..., sample_with=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmcmc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# [0] to return only samples, not acceptance probabilities.\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m samples[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/sbi/samplers/rejection/rejection.py:288\u001b[39m, in \u001b[36maccept_reject_sample\u001b[39m\u001b[34m(proposal, accept_reject_fn, num_samples, num_xos, show_progress_bars, warn_acceptance, sample_for_correction_factor, max_sampling_batch_size, proposal_sampling_kwargs, alternative_method, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m num_samples_possible = \u001b[32m0\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m num_remaining > \u001b[32m0\u001b[39m:\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# Sample and reject.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     candidates = \u001b[43mproposal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mproposal_sampling_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# SNPE-style rejection-sampling when the proposal is the neural net.\u001b[39;00m\n\u001b[32m    293\u001b[39m     are_accepted = accept_reject_fn(candidates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/sbi/neural_nets/estimators/nflows_flow.py:137\u001b[39m, in \u001b[36mNFlowsFlow.sample\u001b[39m\u001b[34m(self, sample_shape, condition)\u001b[39m\n\u001b[32m    134\u001b[39m condition_batch_dim = condition.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    135\u001b[39m num_samples = torch.Size(sample_shape).numel()\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m samples = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Change from Nflows' convention of (batch_dim, sample_dim, *event_shape) to\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# (sample_dim, batch_dim, *event_shape) (PyTorch + SBI).\u001b[39;00m\n\u001b[32m    140\u001b[39m samples = samples.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/nflows/distributions/base.py:65\u001b[39m, in \u001b[36mDistribution.sample\u001b[39m\u001b[34m(self, num_samples, context, batch_size)\u001b[39m\n\u001b[32m     62\u001b[39m     context = torch.as_tensor(context)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check.is_positive_int(batch_size):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/nflows/flows/base.py:54\u001b[39m, in \u001b[36mFlow._sample\u001b[39m\u001b[34m(self, num_samples, context)\u001b[39m\n\u001b[32m     49\u001b[39m     noise = torchutils.merge_leading_dims(noise, num_dims=\u001b[32m2\u001b[39m)\n\u001b[32m     50\u001b[39m     embedded_context = torchutils.repeat_rows(\n\u001b[32m     51\u001b[39m         embedded_context, num_reps=num_samples\n\u001b[32m     52\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m samples, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedded_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedded_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Split the context dimension from sample dimension.\u001b[39;00m\n\u001b[32m     58\u001b[39m     samples = torchutils.split_leading_dim(samples, shape=[-\u001b[32m1\u001b[39m, num_samples])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/nflows/transforms/base.py:60\u001b[39m, in \u001b[36mCompositeTransform.inverse\u001b[39m\u001b[34m(self, inputs, context)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m     funcs = (transform.inverse \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transforms[::-\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cascade\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuncs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/nflows/transforms/base.py:50\u001b[39m, in \u001b[36mCompositeTransform._cascade\u001b[39m\u001b[34m(inputs, funcs, context)\u001b[39m\n\u001b[32m     48\u001b[39m total_logabsdet = inputs.new_zeros(batch_size)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m funcs:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     outputs, logabsdet = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     total_logabsdet += logabsdet\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, total_logabsdet\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/nflows/transforms/autoregressive.py:47\u001b[39m, in \u001b[36mAutoregressiveTransform.inverse\u001b[39m\u001b[34m(self, inputs, context)\u001b[39m\n\u001b[32m     45\u001b[39m logabsdet = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_inputs):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     autoregressive_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautoregressive_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     outputs, logabsdet = \u001b[38;5;28mself\u001b[39m._elementwise_inverse(\n\u001b[32m     49\u001b[39m         inputs, autoregressive_params\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, logabsdet\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/nflows/transforms/made.py:277\u001b[39m, in \u001b[36mMADE.forward\u001b[39m\u001b[34m(self, inputs, context)\u001b[39m\n\u001b[32m    275\u001b[39m temps = \u001b[38;5;28mself\u001b[39m.initial_layer(inputs)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     temps += \u001b[38;5;28mself\u001b[39m.activation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_residual_blocks:\n\u001b[32m    279\u001b[39m     temps = \u001b[38;5;28mself\u001b[39m.activation(temps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/electron/Professionnel/[2022-26] ENS/Stages/[4A] UPMC-SU/sbi-hackathon-2026/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "compare_npe([0, 1, 2, 3, 4, 5, 6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
