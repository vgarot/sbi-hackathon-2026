{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c225506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teddy.data.dataset import MsaLabels\n",
    "from teddy.lightning.datamodule import BDS_datamodule\n",
    "import teddy.data.Alphabet as alphabet\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gc\n",
    "\n",
    "from sbi.analysis import pairplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sbi.neural_nets.net_builders import build_nsf\n",
    "from sbi.neural_nets.embedding_nets import CausalCNNEmbedding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "alphabet_instance = alphabet.Alphabet(list( \"ATGCX-\"))\n",
    "\n",
    "train_ratio = 0.8\n",
    "batch_size = 1\n",
    "val_batch_size = batch_size\n",
    "\n",
    "# Optimize dataloader performance\n",
    "num_workers = 8  # Parallel data loading workers\n",
    "\n",
    "#msa = MsaLabels(dir = \"data/example/seq\", alphabet=alphabet_instance, limit_size=200)\n",
    "data = BDS_datamodule(data_dir = \"data/example/seq\", \n",
    "                      alphabet=alphabet_instance, \n",
    "                      limit_size=200,\n",
    "                      max_sites_len=200,\n",
    "                      train_ratio=train_ratio, \n",
    "                      val_batch_size=val_batch_size, \n",
    "                      batch_size=batch_size,\n",
    "                      num_workers=num_workers,\n",
    "                      prefetch_factor=1,\n",
    "                      persistent_workers=False, \n",
    "                      pin_memory=False, \n",
    "                      cache_dir = \"data/cache\")\n",
    "\n",
    "data.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e2e08",
   "metadata": {},
   "source": [
    "# Setting up dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167eaf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 127)\n",
      "(64, 63)\n",
      "(20, 31)\n"
     ]
    }
   ],
   "source": [
    "# Density estimator with first sequence and prior just for the dimensionality\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define training params\n",
    "learning_rate = 5e-4\n",
    "validation_fraction = 0.1  # 10% of the data will be used for validation\n",
    "stop_after_epochs = 5  # Stop training after 5 epochs with no improvement\n",
    "max_num_epochs = 2**31 - 1\n",
    "\n",
    "train_loader = data.train_dataloader()\n",
    "val_loader = data.val_dataloader()\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "\n",
    "x_batch = batch[0][0]\n",
    "theta_batch = batch[1]\n",
    "\n",
    "embedding_net = CausalCNNEmbedding(  # SOLUTION\n",
    "    input_shape=(batch[0][0].shape[1],),      # 20000 timepoints\n",
    "    in_channels=1,            # 1 channel: sequence data\n",
    "    output_dim=20,            # Compress to 20 learned summary features\n",
    "    num_conv_layers=5,        # Number of dilated causal conv layers\n",
    "    kernel_size=2,            # Kernel size for convolutions\n",
    ")\n",
    "\n",
    "density_estimator = build_nsf(theta_batch, x_batch, z_score_y=\"none\", z_score_x=\"none\", embedding_net=embedding_net) # theta batch dimension, x batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3477989",
   "metadata": {},
   "source": [
    "# Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 10.2458, Val loss: 5.2677, Stops in: 5'\n",
      "Epoch 2: Train loss: 4.6456, Val loss: 3.8586, Stops in: 5'\n",
      "Epoch 3: Train loss: 3.8670, Val loss: 3.7096, Stops in: 5'\n",
      "Epoch 4: Train loss: 3.7190, Val loss: 3.5880, Stops in: 5'\n",
      "Epoch 5: Train loss: 3.6096, Val loss: 3.5738, Stops in: 5'\n",
      "Epoch 6: Train loss: 3.5036, Val loss: 3.4765, Stops in: 5'\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(list(density_estimator.parameters()), lr=learning_rate)\n",
    "\n",
    "# Clean up initialization batch\n",
    "# del train_iter, batch, x_batch, theta_batch\n",
    "gc.collect()\n",
    "\n",
    "# === OPTIMIZED TRAINING LOOP ===\n",
    "epoch = 0\n",
    "best_val_loss = float(\"Inf\")\n",
    "epochs_since_last_improvement = 0\n",
    "converged = False\n",
    "\n",
    "while epoch <= max_num_epochs and not converged:\n",
    "    # === TRAINING PHASE ===\n",
    "    density_estimator.train()\n",
    "    train_loss_sum = 0\n",
    "    num_train_samples = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Convert batch data to tensors\n",
    "        # x_batch = batch[0][0].squeeze()\n",
    "        # theta_batch = torch.as_tensor(batch[1], dtype=torch.float32).squeeze()\n",
    "        x_batch = batch[0][0]\n",
    "        theta_batch = batch[1]\n",
    "\n",
    "        #print(f\"Training Batch {batch_idx}: x shape {x_batch.shape}, theta shape {theta_batch.shape}\")\n",
    "\n",
    "        # Forward pass and loss computation\n",
    "        optimizer.zero_grad()\n",
    "        train_losses = density_estimator.loss(theta_batch, x_batch)\n",
    "        train_loss = torch.mean(train_losses)\n",
    "        \n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss_sum += train_losses.sum().item()\n",
    "        num_train_samples += theta_batch.size(0)\n",
    "        \n",
    "        # CRITICAL: Immediate cleanup after each batch\n",
    "        del x_batch, theta_batch, train_losses, train_loss\n",
    "        \n",
    "        # Periodic garbage collection during training\n",
    "        if batch_idx % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    epoch += 1\n",
    "    train_loss_average = train_loss_sum / num_train_samples\n",
    "\n",
    "    # === VALIDATION PHASE ===\n",
    "    density_estimator.eval()\n",
    "    val_loss_sum = 0\n",
    "    num_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            x_batch = batch[0][0]\n",
    "            theta_batch = batch[1]\n",
    "            \n",
    "            val_losses = density_estimator.loss(theta_batch, x_batch)\n",
    "            val_loss_sum += val_losses.sum().item()\n",
    "            num_val_samples += theta_batch.size(0)\n",
    "            \n",
    "            # Immediate cleanup\n",
    "            del x_batch, theta_batch, val_losses\n",
    "            \n",
    "            if batch_idx % 5 == 0:\n",
    "                gc.collect()\n",
    "    \n",
    "    val_loss = val_loss_sum / num_val_samples\n",
    "\n",
    "    # === MODEL CHECKPOINTING ===\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_since_last_improvement = 0\n",
    "        # Store only state dict, not entire model\n",
    "        best_model_state_dict = deepcopy(density_estimator.state_dict())\n",
    "    else:\n",
    "        epochs_since_last_improvement += 1\n",
    "\n",
    "    # === CONVERGENCE CHECK ===\n",
    "    if epochs_since_last_improvement > stop_after_epochs - 1:\n",
    "        density_estimator.load_state_dict(best_model_state_dict)\n",
    "        converged = True\n",
    "        print(f'\\nNeural network successfully converged after {epoch} epochs')\n",
    "    else:\n",
    "        print(f\"Epoch {epoch}: Train loss: {train_loss_average:.4f}, Val loss: {val_loss:.4f}, Stops in: {stop_after_epochs - epochs_since_last_improvement}'\")\n",
    "    \n",
    "    # Force garbage collection after each epoch\n",
    "    gc.collect()\n",
    "\n",
    "# === POST-TRAINING CLEANUP ===\n",
    "print(\"\\nCleaning up training resources...\")\n",
    "\n",
    "# Delete dataloaders to free worker memory\n",
    "del val_loader\n",
    "gc.collect()\n",
    "\n",
    "# Optional: If using CUDA, clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Training complete and memory cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9bb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "\n",
    "x_o = torch.flatten(torch.as_tensor(batch[0][0], dtype=torch.float32), start_dim=1)\n",
    "theta_o = torch.as_tensor(batch[1], dtype=torch.float32)\n",
    "\n",
    "print(f\"Shape of x_o: {x_o.shape}            # Must have a batch dimension\")\n",
    "\n",
    "samples = density_estimator.sample((10000,), condition=x_o).detach()\n",
    "print(\n",
    "    f\"Shape of samples: {samples.shape}  # Samples are returned with a batch dimension.\"\n",
    ")\n",
    "\n",
    "samples = samples.squeeze(dim=1)\n",
    "print(f\"Shape of samples: {samples.shape}     # Removed batch dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize posterior with pairplot\n",
    "param_labels = [r\"$R_0$\", r\"$\\delta$\"]\n",
    "\n",
    "fig, axes = pairplot(\n",
    "    samples,\n",
    "    #limits=[[0.05, 0.15], [0.01, 0.03], [0.005, 0.03], [0.005, 0.15]],\n",
    "    labels=param_labels,\n",
    "    figsize=(8, 8),\n",
    "    points=theta_o,  # True parameters\n",
    "    points_colors=\"r\",\n",
    ")\n",
    "plt.suptitle(\"NPE Posterior (sbi)\", y=1.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
