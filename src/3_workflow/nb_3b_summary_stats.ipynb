{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4c3c16",
   "metadata": {},
   "source": [
    "# The Role of Summary Statistics in SBI\n",
    "\n",
    "**Time: ~20 minutes**\n",
    "\n",
    "In Notebook 3, we learned how to use the `sbi` toolbox for Neural Posterior Estimation. Now we'll explore a crucial question:\n",
    "\n",
    "> **How do summary statistics affect the quality of our inference?**\n",
    "\n",
    "In simulation-based inference, we often need to compress high-dimensional simulator outputs (like time series) into lower-dimensional summary statistics. The choice of summary statistics can dramatically impact inference quality!\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "1. **Simple statistics**: Mean, std, max, skew, kurtosis (10 summary stats)\n",
    "2. **Advanced statistics**: Adding autocorrelation to capture temporal dynamics (20 summary stats)\n",
    "3. **Neural embedding**: Let a neural network learn the summary statistics directly from raw time series\n",
    "\n",
    "**Key insight**: More informative summary statistics → tighter posteriors → better parameter recovery!\n",
    "\n",
    "**Documentation**: [sbi.readthedocs.io](https://sbi.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sbi.inference import NPE\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import CausalCNNEmbedding\n",
    "\n",
    "from simulators import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    lotka_volterra_simulator,\n",
    "    simulate,\n",
    ")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e3aab",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: The Lotka-Volterra Predator-Prey Model\n",
    "\n",
    "We continue working with the Lotka-Volterra model from the previous notebooks. This model describes the dynamics of a predator-prey ecosystem with 4 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the prior (same for all experiments)\n",
    "prior = create_lotka_volterra_prior()\n",
    "\n",
    "# True parameters for our \"observed\" data\n",
    "_, theta_o = generate_observed_data()  # theta_o = [0.1, 0.02, 0.01, 0.1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the \"ground truth\" time series\n",
    "time = np.arange(0, 200, 0.1)\n",
    "ts_observed = simulate(theta_o.numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(time, ts_observed[:, 0], color=\"C0\", linewidth=2)\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"Prey Dynamics (Ground Truth)\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(time, ts_observed[:, 1], color=\"C1\", linewidth=2)\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"Predator Dynamics (Ground Truth)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Time series length: {len(ts_observed)} time points\")\n",
    "print(\"!! The challenge: How do we compress this into informative summary statistics? !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for posterior predictive visualization\n",
    "def plot_posterior_predictive(samples, theta_o, ts_observed, time, n_samples=50, title=\"\"):\n",
    "    \"\"\"Plot posterior predictive simulations vs ground truth.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "    # Sample some posterior parameters and simulate\n",
    "    indices = np.random.choice(len(samples), size=n_samples, replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        theta_sample = samples[idx].numpy()\n",
    "        ts_sample = simulate(theta_sample)\n",
    "        axes[0].plot(time, ts_sample[:, 0], color=\"C0\", alpha=0.15, linewidth=0.5)\n",
    "        axes[1].plot(time, ts_sample[:, 1], color=\"C1\", alpha=0.15, linewidth=0.5)\n",
    "\n",
    "    # Plot ground truth\n",
    "    axes[0].plot(time, ts_observed[:, 0], color=\"black\", linewidth=2, label=\"Ground truth\")\n",
    "    axes[1].plot(time, ts_observed[:, 1], color=\"black\", linewidth=2, label=\"Ground truth\")\n",
    "\n",
    "    # Formatting\n",
    "    axes[0].set_xlabel(\"Time (days)\")\n",
    "    axes[0].set_ylabel(\"Population\")\n",
    "    axes[0].set_title(f\"Prey - {title}\")\n",
    "    axes[0].legend()\n",
    "    axes[0].plot([], [], color=\"C0\", alpha=0.5, label=\"Posterior samples\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].set_xlabel(\"Time (days)\")\n",
    "    axes[1].set_ylabel(\"Population\")\n",
    "    axes[1].set_title(f\"Predator - {title}\")\n",
    "    axes[1].plot([], [], color=\"C1\", alpha=0.5, label=\"Posterior samples\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0892a93",
   "metadata": {},
   "source": [
    "---\n",
    "## Think First!\n",
    "\n",
    "Before we start experimenting, let's think about summary statistics:\n",
    "\n",
    "**Question 1**: Why do we need summary statistics in SBI? Why not use the raw time series directly?\n",
    "\n",
    "**Question 2**: What properties should \"good\" summary statistics have?\n",
    "\n",
    "**Question 3**: What information might simple statistics (mean, std, max) miss about oscillatory dynamics?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "1. **Why summary statistics?**\n",
    "   - Raw simulator output can be very high-dimensional (e.g., 2000 time points × 2 populations = 4000 dimensions!)\n",
    "   - Neural networks struggle with such high-dimensional inputs\n",
    "   - Well-chosen summary statistics can capture the essential information in a compact form\n",
    "   - However: poor summary statistics can lose crucial information!\n",
    "\n",
    "2. **Good summary statistics should:**\n",
    "   - Be **sufficient**: capture all information relevant for distinguishing parameters\n",
    "   - Be **low-dimensional**: easier for the neural network to learn\n",
    "   - Be **robust**: not too sensitive to noise\n",
    "   - Be **informative**: different parameters should produce different statistics\n",
    "\n",
    "3. **What simple statistics miss:**\n",
    "   - **Temporal structure**: mean/std don't capture oscillation frequency or phase\n",
    "   - **Autocorrelation**: how values at time t relate to values at time t+k\n",
    "   - **Periodicity**: the characteristic period of the predator-prey cycles\n",
    "   - This is why we'll add autocorrelation features as \"advanced\" statistics!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f546af",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Simple Summary Statistics\n",
    "\n",
    "Let's start with basic summary statistics: **mean, std, max, skew, and kurtosis** for each population.\n",
    "\n",
    "This gives us **10 summary statistics** total (5 per population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af87e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate observed data with SIMPLE summary statistics (no autocorrelation)\n",
    "x_o_simple, _ = generate_observed_data(use_autocorrelation=False)\n",
    "\n",
    "print(f\"with x_o_simple we have {x_o_simple.shape[1]} summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b986e0",
   "metadata": {},
   "source": [
    "### Your Task: Train NPE with Simple Statistics\n",
    "\n",
    "Generate training data and train NPE using the simple summary statistics.\n",
    "\n",
    "**Hints**:\n",
    "- Use `lotka_volterra_simulator(theta, use_autocorrelation=False)` for simple stats\n",
    "- Use the same workflow as in Notebook 3: sample -> simulate -> train -> sample posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGenerate training data with simple statistics\n",
    "num_simulations = 2_000\n",
    "\n",
    "# TODO: Sample parameters from the prior\n",
    "# theta_simple = ...\n",
    "\n",
    "# TODO: Simulate with simple summary statistics\n",
    "# x_simple = ...\n",
    "\n",
    "print(f\"Training data shapes: theta={theta_simple.shape}, x={x_simple.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NPE with simple statistics\n",
    "# TODO: Create NPE, append simulations, and train\n",
    "# npe_simple = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07498d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build posterior and sample\n",
    "# posterior_simple = ...\n",
    "# samples_simple = ...\n",
    "\n",
    "print(f\"Posterior samples shape: {samples_simple.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2c55c",
   "metadata": {},
   "source": [
    "### Evaluate Part 1: How good are our results?\n",
    "\n",
    "Let's visualize the posterior and check if simulations from posterior samples match the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for Part 1: Simple Statistics\n",
    "param_labels = [r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"]\n",
    "limits = [[0.05, 0.15], [0.01, 0.03], [0.005, 0.03], [0.005, 0.15]]\n",
    "\n",
    "fig, axes = pairplot(\n",
    "    samples_simple,\n",
    "    limits=limits,\n",
    "    labels=param_labels,\n",
    "    figsize=(8, 8),\n",
    "    points=theta_o,\n",
    ")\n",
    "plt.suptitle(\"Part 1: Posterior with Simple Statistics (10 features)\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24450947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior Predictive Check for Part 1\n",
    "# Do simulations from the posterior match the observed time series?\n",
    "plot_posterior_predictive(\n",
    "    samples_simple, theta_o, ts_observed, time,\n",
    "    n_samples=50, title=\"Simple Statistics\"\n",
    ")\n",
    "print(\"Notice: The posterior is quite broad - many different dynamics are plausible!\")\n",
    "print(\"This is because simple statistics don't capture the oscillation patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0d347",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Advanced Summary Statistics (with Autocorrelation)\n",
    "\n",
    "The Lotka-Volterra model produces **oscillatory dynamics**. Simple statistics like mean and std don't capture the temporal structure!\n",
    "\n",
    "Let's add **autocorrelation** features that measure how the time series correlates with itself at different time lags. This captures:\n",
    "- The period of oscillations\n",
    "- How quickly correlations decay\n",
    "- The temporal dynamics that distinguish different parameter settings\n",
    "\n",
    "This gives us **20 summary statistics** total (10 per population: 5 moments + 5 autocorrelation lags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate observed data with ADVANCED summary statistics (with autocorrelation)\n",
    "x_o_advanced, _ = generate_observed_data(use_autocorrelation=True)\n",
    "\n",
    "print(f\"with x_o_advanced we have {x_o_advanced.shape[1]} summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e0d69",
   "metadata": {},
   "source": [
    "### Your Task: Train NPE with Advanced Statistics\n",
    "\n",
    "Now train NPE using the advanced summary statistics with autocorrelation.\n",
    "\n",
    "**Hint**: Change `use_autocorrelation=True` in the simulator call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data with advanced statistics\n",
    "# TODO: Sample and simulate with autocorrelation\n",
    "# theta_advanced = ...\n",
    "# x_advanced = ...\n",
    "\n",
    "print(f\"Training data shapes: theta={theta_advanced.shape}, x={x_advanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NPE with advanced statistics\n",
    "# TODO: Create NPE, train\n",
    "# npe_advanced = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build posterior and sample\n",
    "# posterior_advanced = ...\n",
    "# samples_advanced = ...\n",
    "\n",
    "print(f\"Posterior samples shape: {samples_advanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5244c7",
   "metadata": {},
   "source": [
    "### Evaluate Part 2: Did autocorrelation help?\n",
    "\n",
    "Let's see if adding temporal information (autocorrelation) improves our inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b336de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for Part 2: Advanced Statistics\n",
    "fig, axes = pairplot(\n",
    "    samples_advanced,\n",
    "    limits=limits,\n",
    "    labels=param_labels,\n",
    "    figsize=(8, 8),\n",
    "    points=theta_o,\n",
    ")\n",
    "plt.suptitle(\"Part 2: Posterior with Advanced Statistics (20 features)\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aff621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior Predictive Check for Part 2\n",
    "plot_posterior_predictive(\n",
    "    samples_advanced, theta_o, ts_observed, time,\n",
    "    n_samples=50, title=\"Advanced Statistics (with Autocorrelation)\"\n",
    ")\n",
    "print(\"Much better! The posterior samples now capture the oscillation patterns.\")\n",
    "print(\"Autocorrelation features helped the network understand temporal dynamics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a63e9",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Neural Embedding Networks (Learning Summary Statistics)\n",
    "\n",
    "What if we could **learn** the summary statistics directly from data?\n",
    "\n",
    "Instead of hand-crafting summary statistics, we can use a **neural embedding network** that:\n",
    "1. Takes the raw time series as input\n",
    "2. Learns to compress it into informative features\n",
    "3. Feeds those features to the posterior estimator\n",
    "\n",
    "The posterior approximation becomes: $q_\\phi\\big(\\theta \\mid f_\\lambda(x_o)\\big)$ where:\n",
    "- $\\phi$ are the parameters of the density estimator (normalizing flow)\n",
    "- $\\lambda$ are the parameters of the embedding network\n",
    "- Both are **jointly learned** during training!\n",
    "\n",
    "Workflow:\n",
    "1. Create the embedding network\n",
    "2. Create the density estimator WITH the embedding\n",
    "3. Pass to NPE\n",
    "\n",
    "`sbi` provides several pre-configured embedding networks:\n",
    "- `FCEmbedding`: Fully-connected MLP\n",
    "- `CNNEmbedding`: Convolutional neural network (for images)\n",
    "- `CausalCNNEmbedding`: Causal 1D CNN based on WaveNet (for time series) ← **We'll use this!**\n",
    "- `PermutationInvariantEmbedding`: For iid trial-based data\n",
    "\n",
    "See: [sbi embedding networks tutorial](https://sbi.readthedocs.io/en/stable/advanced_tutorials/04_embedding_networks.html) and [CausalCNNEmbedding source](https://github.com/sbi-dev/sbi/blob/main/sbi/neural_nets/embedding_nets/causal_cnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daca64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a simulator that returns RAW time series (not summary statistics)\n",
    "# CausalCNNEmbedding expects shape: (batch_size, in_channels, num_timepoints)\n",
    "def raw_timeseries_simulator(params: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Simulate and return raw time series for CausalCNN embedding.\"\"\"\n",
    "    params = params.unsqueeze(0) if params.ndim == 1 else params\n",
    "    # simulate() returns (timepoints, 2) for each parameter set\n",
    "    # Stack to get (batch, timepoints, channels), then transpose to (batch, channels, timepoints)\n",
    "    ts = torch.tensor(np.stack([simulate(p.numpy()) for p in params]), dtype=torch.float32)\n",
    "    return ts.permute(0, 2, 1)  # (batch, channels=2, timepoints=2000)\n",
    "\n",
    "# Test it\n",
    "test_ts = raw_timeseries_simulator(theta_o)\n",
    "print(f\"Raw time series shape: {test_ts.shape}\")\n",
    "print(\"  → (batch_size, in_channels=2, num_timepoints=2000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d781c49",
   "metadata": {},
   "source": [
    "### Your Task: Train NPE with Causal CNN Embedding\n",
    "\n",
    "Now train NPE using the raw time series with a **CausalCNNEmbedding** network.\n",
    "\n",
    "The CausalCNN is based on the WaveNet architecture and uses dilated causal convolutions to capture temporal patterns at multiple scales.\n",
    "\n",
    "**Steps**:\n",
    "1. Generate training data as raw time series (shape: `batch_size × in_channels × num_timepoints`)\n",
    "2. Create a `CausalCNNEmbedding` network that compresses time series to fixed-size vectors\n",
    "3. Create the density estimator \n",
    "4. Create NPE \n",
    "5. Train and sample as usual\n",
    "\n",
    "**Key parameters for `CausalCNNEmbedding`**:\n",
    "- `input_shape`: Tuple with number of timepoints, e.g. `(2000,)`\n",
    "- `in_channels`: Number of input channels (2 for prey + predator)\n",
    "- `output_dim`: Size of the learned summary representation\n",
    "- `num_conv_layers`, `kernel_size`: Control network depth and receptive field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22aa9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data as raw time series\n",
    "# Note: We use fewer simulations here as training with embeddings is slower\n",
    "num_simulations_cnn = 2_000\n",
    "\n",
    "# TODO: Sample parameters\n",
    "# theta_cnn = ...\n",
    "\n",
    "# TODO: Simulate raw time series\n",
    "# x_cnn = ...\n",
    "\n",
    "# Also generate observed data as raw time series\n",
    "x_o_cnn = raw_timeseries_simulator(theta_o)\n",
    "\n",
    "print(f\"Training data shapes: theta={theta_cnn.shape}, x={x_cnn.shape}\")\n",
    "print(f\"Observed data shape: {x_o_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d27dc4",
   "metadata": {},
   "source": [
    "For reference on the CausalCNN embedding network, see the [documentation](https://sbi.readthedocs.io/en/latest/reference/_autosummary/sbi.neural_nets.embedding_nets.CausalCNNEmbedding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941826e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Create the CausalCNN embedding network\n",
    "# The embedding network compresses time series → fixed-size vector\n",
    "#\n",
    "# Input shape:  (batch_size, in_channels=2, num_timepoints=2000)\n",
    "# Output shape: (batch_size, output_dim=20)\n",
    "\n",
    "# TODO: Create CausalCNN embedding network\n",
    "# embedding_net = ...\n",
    "\n",
    "print(f\"CausalCNN Embedding network created!\")\n",
    "print(f\"  - Input: time series of shape (batch, 2, 2000)\")\n",
    "print(f\"  - Output: embedding of shape (batch, 20)\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in embedding_net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3fcf4",
   "metadata": {},
   "source": [
    "For reference, on how to use embedding networks, see [this](https://sbi.readthedocs.io/en/stable/advanced_tutorials/04_embedding_networks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3de456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create density estimator WITH the embedding network\n",
    "# The embedding_net is passed to posterior_nn() and will be jointly trained!\n",
    "\n",
    "# TODO: Create the neural density estimator with embedding\n",
    "# density_estimator = posterior_nn(...)\n",
    "\n",
    "# Step 3 - Create NPE with the custom density estimator\n",
    "# TODO: Create NPE inference object. Check how to embedd the density estimator into the NPE.\n",
    "# npe_cnn = ...\n",
    "\n",
    "# Step 4 - Train (embedding and flow are learned jointly!)\n",
    "print(\"Training NPE with CausalCNN embedding...\")\n",
    "print(\"(The embedding network learns to extract informative features from raw time series)\")\n",
    "# npe_cnn...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36da890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Build posterior and sample\n",
    "# Note: We pass the RAW time series x_o_cnn - the embedding network handles compression!\n",
    "# posterior_cnn = ...\n",
    "# samples_cnn = ...\n",
    "\n",
    "print(f\"Posterior samples shape: {samples_cnn.shape}\")\n",
    "print(f\"\\nThe CausalCNN embedding automatically compressed the {x_o_cnn.shape[-1]}-step time series\")\n",
    "print(f\"into a 20-dimensional learned summary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1919269",
   "metadata": {},
   "source": [
    "### Evaluate Part 3: How well did the neural embedding learn?\n",
    "\n",
    "Let's see if the CausalCNN embedding network learned useful features from the raw time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55066092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for Part 3: CausalCNN Embedding\n",
    "fig, axes = pairplot(\n",
    "    samples_cnn,\n",
    "    limits=limits,\n",
    "    labels=param_labels,\n",
    "    figsize=(8, 8),\n",
    "    points=theta_o,\n",
    ")\n",
    "plt.suptitle(\"Part 3: Posterior with CausalCNN Embedding (learned features)\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior Predictive Check for Part 3\n",
    "plot_posterior_predictive(\n",
    "    samples_cnn, theta_o, ts_observed, time,\n",
    "    n_samples=50, title=\"CausalCNN Embedding (learned)\"\n",
    ")\n",
    "print(\"The CausalCNN network learned to extract relevant features from raw time series!\")\n",
    "print(\"No hand-crafted summary statistics needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d0f91",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing All Three Approaches\n",
    "\n",
    "Now let's visualize and compare the posteriors from all three methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a151536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed pairplot comparison\n",
    "fig, axes = pairplot(\n",
    "    [samples_simple, samples_advanced, samples_cnn],\n",
    "    limits=limits,\n",
    "    labels=param_labels,\n",
    "    figsize=(10, 10),\n",
    "    points=theta_o,\n",
    "    diag=\"hist\",\n",
    "    upper=\"scatter\",\n",
    ")\n",
    "# Add legend manually\n",
    "fig.legend(\n",
    "    [\"Simple Stats\", \"Advanced Stats\", \"CausalCNN Embedding\"],\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(0.95, 0.95),\n",
    ")\n",
    "plt.suptitle(\"Posterior Comparison: Effect of Summary Statistics\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16f280",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Approach | Summary Stats | Pros | Cons |\n",
    "|----------|---------------|------|------|\n",
    "| **Simple** | Mean, std, max, skew, kurtosis | Fast, interpretable | Misses temporal structure |\n",
    "| **Advanced** | + Autocorrelation lags | Captures oscillations | Requires domain knowledge |\n",
    "| **Neural (CausalCNN)** | Learned from data | No hand-crafting needed | More training data required |\n",
    "\n",
    "### The `sbi` Pattern for Embedding Networks\n",
    "\n",
    "```python\n",
    "# 1. Create embedding network\n",
    "embedding_net = CausalCNNEmbedding(input_shape=(...,), in_channels=..., output_dim=...)\n",
    "\n",
    "# 2. Create density estimator with embedding\n",
    "density_estimator = posterior_nn(model=\"maf\", embedding_net=embedding_net)\n",
    "\n",
    "# 3. Create inference object\n",
    "inferer = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "# 4. Train (embedding and flow learned jointly!)\n",
    "inferer.append_simulations(theta, x).train()\n",
    "```\n",
    "\n",
    "### Main Insights\n",
    "\n",
    "1. **Summary statistics matter!** The choice of summary statistics dramatically affects inference quality.\n",
    "\n",
    "2. **Domain knowledge helps**: Adding autocorrelation (which captures temporal dynamics) significantly improves inference for oscillatory systems.\n",
    "\n",
    "3. **Neural embeddings are powerful**: When you don't know the \"right\" summary statistics, let a neural network learn them!\n",
    "\n",
    "4. **Trade-offs exist**:\n",
    "   - Simple stats: Fast and interpretable, but may lose information\n",
    "   - Neural embeddings: Flexible, but need more data and computation\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "- **Simple statistics**: Quick prototyping, well-understood systems\n",
    "- **Domain-specific statistics**: When you know what features matter\n",
    "- **Neural embeddings**: Complex data (images, time series), unknown optimal summaries\n",
    "\n",
    "For more details, see the [sbi tutorial on embedding networks](https://sbi.readthedocs.io/en/stable/advanced_tutorials/04_embedding_networks.html) and [CausalCNNEmbedding source](https://github.com/sbi-dev/sbi/blob/main/sbi/neural_nets/embedding_nets/causal_cnn.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3c2f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Goals\n",
    "\n",
    "After completing this notebook, you should be able to:\n",
    "\n",
    "- ✅ Explain why summary statistics are important in SBI\n",
    "- ✅ Compare the effect of different summary statistics on inference quality\n",
    "- ✅ Use autocorrelation features for time series data\n",
    "- ✅ Implement neural embedding networks (CausalCNN) for learning summary statistics\n",
    "- ✅ Make informed decisions about which summary statistics to use for your problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
