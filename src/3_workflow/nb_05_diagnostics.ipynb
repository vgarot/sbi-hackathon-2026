{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa705a2e",
   "metadata": {},
   "source": [
    "# Diagnostics for Neural Posterior Estimation\n",
    "\n",
    "**Time: ~10 minutes**\n",
    "\n",
    "In the previous notebooks, we:\n",
    "1. **Notebook 3**: Learned the `sbi` workflow for NPE\n",
    "2. **Notebook 4**: Explored different summary statistics\n",
    "\n",
    "Now comes a critical question:\n",
    "\n",
    "> **How do we know if our posterior is trustworthy?**\n",
    "\n",
    "Neural networks can fail silently — they might produce confident-looking but wrong results! Before using our inference for real decisions, we need **diagnostic checks**.\n",
    "\n",
    "## What We'll Learn\n",
    "\n",
    "1. **Training Diagnostics**: Did the neural network converge properly?\n",
    "2. **Posterior Predictive Checks**: Can we recreate the observed data from our posterior?\n",
    "3. **Simulation-Based Calibration (SBC)**: Are our uncertainty estimates reliable?\n",
    "\n",
    "**Documentation**: [sbi Bayesian workflow](https://sbi.readthedocs.io/en/stable/tutorials/01_Bayesian_workflow.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d43fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sbi.inference import NPE, simulate_for_sbi\n",
    "from sbi.analysis import pairplot, sbc_rank_plot\n",
    "from sbi.diagnostics import run_sbc, check_sbc\n",
    "\n",
    "from simulators import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    lotka_volterra_simulator,\n",
    "    simulate,\n",
    ")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095903b",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Quick NPE Training\n",
    "\n",
    "First, let's quickly train an NPE on the Lotka-Volterra model (using what we learned in Notebook 3).\n",
    "\n",
    "**Just run these cells** — we'll focus on diagnostics, not training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ea91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup prior and observed data\n",
    "prior = create_lotka_volterra_prior()\n",
    "x_o, theta_o = generate_observed_data(use_autocorrelation=True)\n",
    "\n",
    "# For visualization later\n",
    "time = np.arange(0, 200, 0.1)\n",
    "ts_observed = simulate(theta_o.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8984510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NPE\n",
    "num_simulations = 1_000\n",
    "\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = lotka_volterra_simulator(theta, use_autocorrelation=True)\n",
    "\n",
    "npe = NPE(prior)\n",
    "npe.append_simulations(theta, x).train()\n",
    "\n",
    "posterior = npe.build_posterior()\n",
    "samples = posterior.sample((10_000,), x=x_o)\n",
    "\n",
    "print(f\"\\nNPE trained! Posterior samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457e18a",
   "metadata": {},
   "source": [
    "---\n",
    "## Think First!\n",
    "\n",
    "Before we run diagnostics, let's think about why they matter:\n",
    "\n",
    "**Question 1**: Why do we need diagnostics for NPE? Can't we just trust the posterior samples?\n",
    "\n",
    "**Question 2**: What is a \"posterior predictive check\"? What should we expect if inference worked well?\n",
    "\n",
    "**Question 3**: If our posterior says \"90% credible interval\", how can we verify this is actually correct?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "1. **Why diagnostics?**\n",
    "   - Neural networks can fail silently — they might give confident but wrong answers\n",
    "   - The posterior might be too narrow (overconfident) or too wide (uninformative)\n",
    "   - We can't know if inference worked without checking!\n",
    "\n",
    "2. **Posterior predictive check:**\n",
    "   - Sample parameters from the posterior → simulate new data → compare to observed data\n",
    "   - If inference worked well, simulated data should look similar to the observation\n",
    "   - The observed data should fall within the distribution of posterior simulations\n",
    "\n",
    "3. **Verifying calibration:**\n",
    "   - Use **Simulation-Based Calibration (SBC)**\n",
    "   - Run many \"fake experiments\" where we know the true parameters\n",
    "   - Check: Does the 90% credible interval contain the truth ~90% of the time?\n",
    "   - If yes → our uncertainty estimates are reliable!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a9731",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Training Diagnostics\n",
    "\n",
    "**The question**: Did the neural network converge during training?\n",
    "\n",
    "Before we trust the posterior, we should verify that training went well:\n",
    "- **Training loss** should decrease and stabilize\n",
    "- **Validation loss** should not increase (no overfitting)\n",
    "- Both should converge to a plateau\n",
    "\n",
    "The `sbi` package stores training statistics that we can visualize with `npe.summary`. \n",
    "\n",
    "**Documentation**: See the [sbi Bayesian workflow tutorial](https://sbi.readthedocs.io/en/stable/tutorials/01_Bayesian_workflow.html) for more details on training diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a435a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress using npe.summary\n",
    "# The summary dict contains training and validation loss\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "# Plot training and validation loss\n",
    "# train_loss = npe....\n",
    "# val_loss = npe....\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# TODO plot function\n",
    "\n",
    "print(\"Check: Both loss curves should increase and stabilize (converge).\")\n",
    "print(\"   If validation loss decreases → overfitting (need more data or regularization)\")\n",
    "print(\"   If loss curves are still increasing → train longer (increase max_num_epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fa319",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Posterior Predictive Check\n",
    "\n",
    "**The idea**: If our posterior is good, then data simulated from posterior parameters should look like the observed data.\n",
    "\n",
    "**The process**:\n",
    "1. Sample parameters from the posterior: θ ~ q(θ|x_o)\n",
    "2. Simulate new data for each parameter: x_sim ~ p(x|θ)\n",
    "3. Compare x_sim to x_o — they should be similar!\n",
    "\n",
    "This is the most intuitive diagnostic: *\"Can our posterior explain what we observed?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554684db",
   "metadata": {},
   "source": [
    "### Your Task: Run a Posterior Predictive Check\n",
    "\n",
    "Use the `plot_posterior_predictive` function to visualize how well simulations from the posterior match the observed data.\n",
    "\n",
    "**Question**: Do the posterior simulations capture the observed dynamics? Are they too spread out or too narrow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior Predictive Check\n",
    "# TODO: implement posterior preditive check function\n",
    "\n",
    "\n",
    "print(\"Check: Do the posterior curves cluster around the black 'observed' line?\")\n",
    "print(\"   If yes → the posterior can explain the observation!\")\n",
    "print(\"   If no  → something might be wrong with inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53784a94",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Simulation-Based Calibration (SBC)\n",
    "\n",
    "**The question**: When our posterior says \"90% credible interval\", is it correct 90% of the time?\n",
    "\n",
    "**The method**:\n",
    "1. Sample \"true\" parameters from the prior: θ* ~ p(θ)\n",
    "2. Simulate fake observations: x* ~ p(x|θ*)\n",
    "3. Run inference to get posterior: q(θ|x*)\n",
    "4. Check: Where does θ* fall in the posterior?\n",
    "5. Repeat many times and check if θ* ranks are uniformly distributed\n",
    "\n",
    "**Why uniform?** If the posterior is well-calibrated, the true parameter should be equally likely to fall anywhere in the posterior distribution.\n",
    "\n",
    "### What Can Go Wrong?\n",
    "\n",
    "| Rank Distribution | Problem | Meaning |\n",
    "|-------------------|---------|----------|\n",
    "| **∩-shaped** (hump in middle) | Underconfident | Posteriors too wide |\n",
    "| **∪-shaped** (humps at edges) | Overconfident | Posteriors too narrow |\n",
    "| **Skewed** | Biased | Systematic error in one direction |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675fcae",
   "metadata": {},
   "source": [
    "### Your Task: Run SBC\n",
    "\n",
    "Complete the code below to run simulation-based calibration.\n",
    "\n",
    "**Hints**:\n",
    "- Use `simulate_for_sbi(simulator, prior, num_simulations)` to generate test data\n",
    "- Use `run_sbc(thetas, xs, posterior, num_posterior_samples)` to compute ranks\n",
    "- Use `sbc_rank_plot(ranks, num_posterior_samples)` to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Generate test data for SBC\n",
    "# We simulate new (theta, x) pairs to test if our posterior is well-calibrated\n",
    "\n",
    "num_sbc_runs = 200  # Number of test cases (use 500+ in practice)\n",
    "num_posterior_samples = 1000  # Samples per posterior\n",
    "\n",
    "print(f\"Running SBC with {num_sbc_runs} test cases...\")\n",
    "print(\"(This tests: 'Are our credible intervals trustworthy?')\\n\")\n",
    "\n",
    "# TODO for students: Generate test data using simulate_for_sbi\n",
    "# Hint: we need (theta, x) pairs from the prior\n",
    "\n",
    "def sbc_simulator(theta):\n",
    "    return lotka_volterra_simulator(theta, use_autocorrelation=True)\n",
    "\n",
    "thetas_sbc, xs_sbc = simulate_for_sbi(\n",
    "    sbc_simulator,\n",
    "    prior,\n",
    "    num_simulations=num_sbc_runs,\n",
    ")\n",
    "\n",
    "print(f\"Generated {num_sbc_runs} test (θ, x) pairs\")\n",
    "print(f\"  thetas shape: {thetas_sbc.shape}\")\n",
    "print(f\"  xs shape: {xs_sbc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d13f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SBC to compute ranks\n",
    "# For each test case, we check where the true theta falls in the posterior\n",
    "\n",
    "# TODO: Use run_sbc to compute ranks\n",
    "\n",
    "# Hint: run_sbc(thetas, xs, posterior, num_posterior_samples, reduce_fns=\"marginals\")\n",
    "# ranks, dap_samples = run_sbc\n",
    "\n",
    "print(f\"\\nComputed ranks for {num_sbc_runs} test cases\")\n",
    "print(f\"Ranks shape: {ranks.shape} (one rank per parameter per test case)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd12ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SBC results\n",
    "# Well-calibrated posteriors → uniform rank distribution (flat histogram)\n",
    "\n",
    "param_labels = [r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"]\n",
    "\n",
    "# TODO: Use sbc_rank_plot to visualize\n",
    "# fig, ax = sbc_rank_plot(\n",
    "\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"   - Flat histogram → well-calibrated posterior\")\n",
    "print(\"   - ∩-shaped → underconfident (posteriors too wide)\")\n",
    "print(\"   - ∪-shaped → overconfident (posteriors too narrow)\")\n",
    "print(\"   - Skewed → biased estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d0ce6",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Diagnostic Toolkit\n",
    "\n",
    "| Diagnostic | What it checks | What to look for |\n",
    "|------------|----------------|------------------|\n",
    "| **Training Loss** | Did the network converge? | Loss decreases and stabilizes |\n",
    "| **Posterior Predictive** | Can posterior explain observations? | Simulations should match observed data |\n",
    "| **SBC** | Are credible intervals calibrated? | Uniform rank histograms |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always validate** — Neural networks can fail silently\n",
    "2. **Training diagnostics** — Check loss curves for convergence and overfitting\n",
    "3. **Posterior predictive checks** — Intuitive sanity check: \"Does this make sense?\"\n",
    "4. **SBC for calibration** — Quantitative check: \"Can we trust the uncertainties?\"\n",
    "5. **Iterate if needed** — Poor diagnostics → adjust and retrain\n",
    "\n",
    "### The `sbi` Pattern for Diagnostics\n",
    "\n",
    "```python\n",
    "# Training diagnostics\n",
    "train_losses = npe.summary[\"training_log_probs\"]\n",
    "val_losses = npe.summary[\"validation_log_probs\"]\n",
    "\n",
    "# Posterior predictive check\n",
    "theta_samples = posterior.sample((N,), x=x_o)\n",
    "x_predicted = simulator(theta_samples)\n",
    "# Compare x_predicted to x_o\n",
    "\n",
    "# Simulation-based calibration\n",
    "from sbi.diagnostics import run_sbc, check_sbc\n",
    "from sbi.analysis import sbc_rank_plot\n",
    "\n",
    "thetas, xs = simulate_for_sbi(simulator, prior, num_simulations=200)\n",
    "ranks, dap = run_sbc(thetas, xs, posterior, num_posterior_samples=1000)\n",
    "sbc_rank_plot(ranks, num_posterior_samples=1000)\n",
    "```\n",
    "\n",
    "For more details, see the [sbi Bayesian workflow tutorial](https://sbi.readthedocs.io/en/stable/tutorials/01_Bayesian_workflow.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "After this notebook, you should be able to:\n",
    "\n",
    "- ✅ Explain why diagnostics are essential for SBI\n",
    "- ✅ Check training convergence using `npe.summary`\n",
    "- ✅ Run posterior predictive checks to validate inference\n",
    "- ✅ Use simulation-based calibration (SBC) to test calibration\n",
    "- ✅ Interpret SBC rank histograms and identify problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
