{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Neural Posterior Estimation (SNPE)\n",
    "\n",
    "**Session 4, Part 3** - Hands-on exercise (~20 min)\n",
    "\n",
    "In the previous notebook, we saw that **NPE struggled** with the Lotka-Volterra problem while **NLE performed well**. The issue: NPE must learn accurate posteriors across the entire prior space, but the true posterior is sharply concentrated.\n",
    "\n",
    "**Solution: Sequential NPE (SNPE)** - Focus simulations where the posterior has mass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: The Problem with Amortized NPE\n",
    "\n",
    "With standard (amortized) NPE:\n",
    "\n",
    "```\n",
    "Sample θ from PRIOR → Simulate x → Train\n",
    "```\n",
    "\n",
    "**Problem:** Most prior samples produce observations far from $x_o$!\n",
    "- Wasted simulations in irrelevant regions\n",
    "- Poor posterior approximation where it matters\n",
    "- Need many simulations to cover the whole prior space\n",
    "\n",
    "### The Sequential Idea\n",
    "\n",
    "```\n",
    "Round 1: Sample θ from PRIOR      → Simulate → Train → Get rough posterior\n",
    "Round 2: Sample θ from POSTERIOR₁ → Simulate → Train → Get better posterior  \n",
    "Round 3: Sample θ from POSTERIOR₂ → Simulate → Train → Get refined posterior\n",
    "...\n",
    "```\n",
    "\n",
    "Each round **focuses simulations** on the region where the posterior has mass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The SNPE API in `sbi`\n",
    "\n",
    "The key pattern is a **loop** where each round:\n",
    "1. Samples from the current `proposal` (prior in round 1, posterior in later rounds)\n",
    "2. Simulates data for those parameters\n",
    "3. Trains the density estimator\n",
    "4. Builds a new posterior and sets it as the next proposal\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "**Initialization:**\n",
    "```python\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "\n",
    "trainer = SNPE(prior)\n",
    "proposal = prior  # Start with prior\n",
    "```\n",
    "\n",
    "**Data generation** (works with any proposal - prior or posterior):\n",
    "```python\n",
    "theta, x = simulate_for_sbi(simulator, proposal, num_simulations, num_workers=NUM_WORKERS)\n",
    "```\n",
    "\n",
    "**Training** (must tell SNPE where samples came from):\n",
    "```python\n",
    "trainer.append_simulations(theta, x, proposal=proposal)  # proposal argument is crucial!\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "**Building posterior and updating proposal:**\n",
    "```python\n",
    "posterior = trainer.build_posterior()\n",
    "proposal = posterior.set_default_x(x_o)  # Makes posterior usable as next proposal\n",
    "```\n",
    "\n",
    "**Key insight:** `posterior.set_default_x(x_o)` conditions the posterior on your observation, so when `simulate_for_sbi` samples from it, you get θ values that are likely given $x_o$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "\n",
    "from simulators import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    lotka_volterra_simulator,\n",
    ")\n",
    "from utils import corner_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total simulations: 20000\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "NUM_ROUNDS = 2\n",
    "NUM_SIMS_PER_ROUND = 10000\n",
    "NUM_WORKERS = 10  # Adjust based on your machine\n",
    "USE_AUTOCORRELATION = True\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(f\"Total simulations: {NUM_ROUNDS * NUM_SIMS_PER_ROUND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameters: [0.1  0.02 0.01 0.1 ]\n",
      "Observation shape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "# Setup prior, simulator, and observation\n",
    "prior = create_lotka_volterra_prior()\n",
    "x_o, theta_true = generate_observed_data(use_autocorrelation=USE_AUTOCORRELATION)\n",
    "\n",
    "simulator = partial(lotka_volterra_simulator, use_autocorrelation=USE_AUTOCORRELATION)\n",
    "\n",
    "print(f\"True parameters: {theta_true.numpy()}\")\n",
    "print(f\"Observation shape: {x_o.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise: Implement Multi-Round SNPE\n",
    "\n",
    "**Your task:** Complete the SNPE training loop below.\n",
    "\n",
    "For each round, you need to:\n",
    "1. Generate training data using `simulate_for_sbi` with the current `proposal`\n",
    "2. Append simulations to the trainer (don't forget the `proposal` argument!)\n",
    "3. Train the density estimator\n",
    "4. Build the posterior and store it\n",
    "5. **Crucially:** Update the `proposal` for the next round using `set_default_x(x_o)`\n",
    "\n",
    "**Hint:** Look at the API section above for the key function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 ===\n",
      "Simulating 10000 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418064b0e89046a382be6a48695c3a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      " Neural network successfully converged after 320 epochs.\n",
      "        -------------------------\n",
      "        ||||| ROUND 1 STATS |||||:\n",
      "        -------------------------\n",
      "        Epochs trained: 320\n",
      "        Best validation performance: -25.3978\n",
      "        -------------------------\n",
      "        \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2591c4e752b8492ab08c64513a2b000b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mean: [0.10027968 0.02015493 0.01004426 0.10010791]\n",
      "Posterior std:  [0.00137833 0.00020186 0.00010054 0.00130889]\n",
      "\n",
      "=== Round 2 ===\n",
      "Simulating 10000 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dbe49be3c14c2cb8205782973e795e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa24cada92434dc5b6455a1aa89e9f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Using SNPE-C with atomic loss\n",
      " Neural network successfully converged after 74 epochs.\n",
      "        -------------------------\n",
      "        ||||| ROUND 2 STATS |||||:\n",
      "        -------------------------\n",
      "        Epochs trained: 74\n",
      "        Best validation performance: 0.1302\n",
      "        -------------------------\n",
      "        \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd31b7f9982e404a9a69c5306b385324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mean: [0.1003072  0.02008603 0.00997723 0.0998537 ]\n",
      "Posterior std:  [7.0991297e-04 1.4114617e-04 9.0472735e-05 8.5990323e-04]\n",
      "\n",
      "=== Done ===\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "trainer = SNPE(prior)\n",
    "proposal = prior  # Start sampling from the prior\n",
    "posteriors = []   # Store posteriors from each round\n",
    "\n",
    "for round_idx in range(NUM_ROUNDS):\n",
    "    print(f\"\\n=== Round {round_idx + 1} ===\")\n",
    "    \n",
    "    # TODO Step 1: Generate training data by sampling from the proposal\n",
    "    # Use simulate_for_sbi(simulator, proposal, NUM_SIMS_PER_ROUND, num_workers=NUM_WORKERS)\n",
    "    print(f\"Simulating {NUM_SIMS_PER_ROUND} samples...\")\n",
    "    theta, x = simulate_for_sbi(simulator, proposal, NUM_SIMS_PER_ROUND, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # TODO Step 2: Append simulations to trainer\n",
    "    # Important: pass proposal=proposal so SNPE knows where samples came from\n",
    "    trainer.append_simulations(theta, x, proposal=proposal)\n",
    "    \n",
    "    # TODO Step 3: Train\n",
    "    print(\"Training...\")\n",
    "    trainer.train(show_train_summary=True)\n",
    "    \n",
    "    # TODO Step 4: Build posterior and store it\n",
    "    posterior = trainer.build_posterior()\n",
    "    posteriors.append(posterior)\n",
    "    \n",
    "    # TODO Step 5: Update proposal for next round\n",
    "    # Use posterior.set_default_x(x_o) to condition on our observation\n",
    "    proposal = posterior.set_default_x(x_o)\n",
    "    \n",
    "    # Quick check (this part is complete)\n",
    "    samples = posterior.sample((1000,), x=x_o)\n",
    "    print(f\"Posterior mean: {samples.mean(dim=0).numpy()}\")\n",
    "    print(f\"Posterior std:  {samples.std(dim=0).numpy()}\")\n",
    "\n",
    "print(\"\\n=== Done ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "theta, x = simulate_for_sbi(simulator, proposal, NUM_SIMS_PER_ROUND, num_workers=NUM_WORKERS)\n",
    "trainer.append_simulations(theta, x, proposal=proposal)\n",
    "trainer.train(show_train_summary=True)\n",
    "posterior = trainer.build_posterior()\n",
    "proposal = posterior.set_default_x(x_o)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample from Final Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fb5a827b444c25bada5fa5996e6cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1feb35dd179148cd9367604e8485b931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get samples from each round for comparison\n",
    "samples_r1 = posteriors[0].sample((5000,), x=x_o)\n",
    "snpe_samples = posteriors[1].sample((5000,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize: Posterior Evolution Across Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples_r2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m param_names = [\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m$\u001b[39m\u001b[33m\\\u001b[39m\u001b[33malpha$\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m$\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbeta$\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m$\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdelta$\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m$\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mgamma$\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m prior_limits = [[prior.base_dist.low[i].item(), prior.base_dist.high[i].item()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m4\u001b[39m)]\n\u001b[32m      5\u001b[39m fig, axes = corner_plot(\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     [samples_r1, \u001b[43msamples_r2\u001b[49m, snpe_samples],\n\u001b[32m      7\u001b[39m     labels=[\u001b[33m\"\u001b[39m\u001b[33mRound 1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRound 2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRound 3\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m     param_names=param_names,\n\u001b[32m      9\u001b[39m     theta_true=theta_true,\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# limits=prior_limits,\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m plt.suptitle(\u001b[33m\"\u001b[39m\u001b[33mSNPE: Posterior Refinement Across Rounds\u001b[39m\u001b[33m\"\u001b[39m, y=\u001b[32m1.02\u001b[39m, fontsize=\u001b[32m12\u001b[39m)\n\u001b[32m     13\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'samples_r2' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize posterior evolution across rounds\n",
    "param_names = [r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"]\n",
    "prior_limits = [[prior.base_dist.low[i].item(), prior.base_dist.high[i].item()] for i in range(4)]\n",
    "\n",
    "fig, axes = corner_plot(\n",
    "    [samples_r1, samples_r2, snpe_samples],\n",
    "    labels=[\"Round 1\", \"Round 2\", \"Round 3\"],\n",
    "    param_names=param_names,\n",
    "    theta_true=theta_true,\n",
    "    # limits=prior_limits,\n",
    ")\n",
    "plt.suptitle(\"SNPE: Posterior Refinement Across Rounds\", y=1.02, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare: SNPE vs NPE vs NLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NPE and NLE samples from previous notebook\n",
    "import os\n",
    "\n",
    "if os.path.exists('npe_nle_samples.pt'):\n",
    "    saved_data = torch.load('npe_nle_samples.pt', weights_only=True)\n",
    "    npe_samples = saved_data['npe_samples']\n",
    "    nle_samples = saved_data['nle_samples']\n",
    "    print(\"Loaded NPE and NLE samples from nb_04\")\n",
    "else:\n",
    "    print(\"Warning: 'npe_nle_samples.pt' not found. Run nb_04 first!\")\n",
    "    print(\"Using placeholder values for comparison.\")\n",
    "    npe_samples = None\n",
    "    nle_samples = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: SNPE vs NPE vs NLE posteriors\n",
    "if npe_samples is not None and nle_samples is not None:\n",
    "    fig, axes = corner_plot(\n",
    "        [npe_samples, nle_samples, snpe_samples],\n",
    "        labels=[\"NPE (20k)\", \"NLE (20k)\", \"SNPE (15k)\"],\n",
    "        param_names=param_names,\n",
    "        theta_true=theta_true,\n",
    "        # limits=prior_limits,\n",
    "    )\n",
    "    plt.suptitle(\"SNPE vs NPE vs NLE: Posterior Comparison\", y=1.02, fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Posterior Predictive Check\n",
    "\n",
    "Let's validate SNPE by checking if simulations from posterior samples match the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulators import simulate\n",
    "\n",
    "# Generate predictive time series from SNPE posterior\n",
    "n_predictive = 50\n",
    "time = np.arange(0, 200, 0.1)\n",
    "\n",
    "# Observed time series\n",
    "ts_observed = simulate(theta_true.numpy())\n",
    "\n",
    "# SNPE predictive time series\n",
    "snpe_predictive_theta = snpe_samples[:n_predictive]\n",
    "ts_snpe = [simulate(snpe_predictive_theta[i].numpy()) for i in range(n_predictive)]\n",
    "\n",
    "# NPE and NLE predictive (if available)\n",
    "if npe_samples is not None:\n",
    "    npe_predictive_theta = npe_samples[:n_predictive]\n",
    "    ts_npe = [simulate(npe_predictive_theta[i].numpy()) for i in range(n_predictive)]\n",
    "if nle_samples is not None:\n",
    "    nle_predictive_theta = nle_samples[:n_predictive]\n",
    "    ts_nle = [simulate(nle_predictive_theta[i].numpy()) for i in range(n_predictive)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictive time series: SNPE vs NPE vs NLE\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "# SNPE predictive\n",
    "ax = axes[0, 0]\n",
    "for ts in ts_snpe:\n",
    "    ax.plot(time, ts[:, 0], color=\"C2\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 0], color=\"k\", linewidth=2, label=\"observed\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"SNPE (6k sims): Prey (Deer)\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[0, 1]\n",
    "for ts in ts_snpe:\n",
    "    ax.plot(time, ts[:, 1], color=\"C2\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 1], color=\"k\", linewidth=2)\n",
    "ax.set_title(\"SNPE (6k sims): Predator (Wolf)\")\n",
    "\n",
    "# NPE predictive\n",
    "if npe_samples is not None:\n",
    "    ax = axes[1, 0]\n",
    "    for ts in ts_npe:\n",
    "        ax.plot(time, ts[:, 0], color=\"C0\", alpha=0.2, linewidth=0.5)\n",
    "    ax.plot(time, ts_observed[:, 0], color=\"k\", linewidth=2, label=\"observed\")\n",
    "    ax.set_ylabel(\"Population\")\n",
    "    ax.set_title(\"NPE (20k sims): Prey (Deer)\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    for ts in ts_npe:\n",
    "        ax.plot(time, ts[:, 1], color=\"C0\", alpha=0.2, linewidth=0.5)\n",
    "    ax.plot(time, ts_observed[:, 1], color=\"k\", linewidth=2)\n",
    "    ax.set_title(\"NPE (20k sims): Predator (Wolf)\")\n",
    "\n",
    "# NLE predictive\n",
    "if nle_samples is not None:\n",
    "    ax = axes[2, 0]\n",
    "    for ts in ts_nle:\n",
    "        ax.plot(time, ts[:, 0], color=\"C1\", alpha=0.2, linewidth=0.5)\n",
    "    ax.plot(time, ts_observed[:, 0], color=\"k\", linewidth=2, label=\"observed\")\n",
    "    ax.set_xlabel(\"Time (days)\")\n",
    "    ax.set_ylabel(\"Population\")\n",
    "    ax.set_title(\"NLE (20k sims): Prey (Deer)\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[2, 1]\n",
    "    for ts in ts_nle:\n",
    "        ax.plot(time, ts[:, 1], color=\"C1\", alpha=0.2, linewidth=0.5)\n",
    "    ax.plot(time, ts_observed[:, 1], color=\"k\", linewidth=2)\n",
    "    ax.set_xlabel(\"Time (days)\")\n",
    "    ax.set_title(\"NLE (20k sims): Predator (Wolf)\")\n",
    "\n",
    "plt.suptitle(\"Posterior Predictive Check: SNPE vs NPE vs NLE\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussion\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Method | Simulations | Posterior Quality | Amortized? |\n",
    "|--------|-------------|-------------------|------------|\n",
    "| NPE | 20,000 | Wider (struggled) | Yes |\n",
    "| NLE | 20,000 | Tight, accurate | Partial |\n",
    "| **SNPE** | **6,000** | Tight, accurate | No |\n",
    "\n",
    "### SNPE Trade-offs\n",
    "\n",
    "**Advantages:**\n",
    "- Much more simulation-efficient for a specific observation\n",
    "- Can achieve tight posteriors even when amortized NPE struggles\n",
    "- Iteratively improves the posterior estimate\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Not amortized**: Must re-run for each new observation\n",
    "- Requires multiple training rounds (more wall-clock time)\n",
    "- Need to choose number of rounds and simulations per round\n",
    "\n",
    "### When to Use SNPE?\n",
    "\n",
    "1. **Expensive simulations**: Each simulation counts, can't afford 100k+ samples\n",
    "2. **Sharp posteriors**: Amortized NPE would need too many simulations\n",
    "3. **Single observation**: You have one $x_o$ and need the best possible posterior\n",
    "4. **Prior is broad**: Large prior-to-posterior contraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### The Sequential SBI Family\n",
    "\n",
    "| Method | What it learns | Sequential variant |\n",
    "|--------|---------------|-------------------|\n",
    "| NPE | $p(\\theta|x)$ | **SNPE** |\n",
    "| NLE | $p(x|\\theta)$ | SNLE |\n",
    "| NRE | $p(x|\\theta)/p(x)$ | SNRE |\n",
    "\n",
    "All sequential methods follow the same principle: **focus simulations where they matter**.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Sequential methods are simulation-efficient** - Important when simulations are expensive\n",
    "2. **Trade-off: efficiency vs amortization** - SNPE gives better posteriors but only for one $x_o$\n",
    "3. **SNPE can rescue NPE** - When amortized NPE struggles with sharp posteriors\n",
    "4. **Choose based on your use case**:\n",
    "   - Many observations, cheap simulations → NPE\n",
    "   - One observation, expensive simulations → SNPE\n",
    "   - Sharp posteriors, need MCMC flexibility → NLE/SNLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've now seen the main SBI methods:\n",
    "- ABC (Session 2)\n",
    "- NPE with neural density estimators (Sessions 2-3)\n",
    "- NLE with MCMC (Session 4)\n",
    "- SNPE for simulation efficiency (Session 4)\n",
    "\n",
    "You're ready for the **hackathon**! Apply these methods to your own problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SBI Hackathon)",
   "language": "python",
   "name": "sbi-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
