{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# From PyTorch to Neural Posterior Estimation\n",
    "\n",
    "In this notebook, we'll build up to **Neural Posterior Estimation (NPE)** step by step:\n",
    "\n",
    "1. **Part 1: PyTorch Primer** - The building blocks of deep learning\n",
    "2. **Part 2: The Inverse Problem** - Why we need distributions, not point estimates\n",
    "3. **Part 3: Neural Posterior Estimation** - Learning to approximate the posterior\n",
    "\n",
    "We'll use a simple 1D example throughout to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: PyTorch Primer\n",
    "\n",
    "Before we dive into NPE, let's review the **5 key ingredients** of deep learning in PyTorch:\n",
    "\n",
    "1. A **dataset**\n",
    "2. A **neural network**\n",
    "3. A **loss function**\n",
    "4. An **optimizer**\n",
    "5. A **training loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-data-intro",
   "metadata": {},
   "source": [
    "### Our Running Example\n",
    "\n",
    "We have a simulator that generates data:\n",
    "$$x = f(\\theta) + \\epsilon = \\theta + 0.3\\sin(2\\pi\\theta) + \\epsilon$$\n",
    "\n",
    "where $\\theta \\sim \\mathcal{U}(0,1)$ is our parameter and $\\epsilon \\sim \\mathcal{N}(0, 0.05)$ is noise.\n",
    "\n",
    "Think of this as a tiny \"simulator\" - given a parameter $\\theta$, it produces an observation $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data: simulate many (theta, x) pairs\n",
    "n_samples = 1000\n",
    "theta = torch.rand((n_samples, 1))  # prior samples\n",
    "noise = torch.randn((n_samples, 1)) * 0.05\n",
    "x = theta + 0.3 * torch.sin(2 * math.pi * theta) + noise  # simulated observations\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(theta, x, s=5, alpha=0.5)\n",
    "plt.xlabel(r\"$\\theta$ (parameter)\")\n",
    "plt.ylabel(r\"$x$ (observation)\")\n",
    "plt.title(\"Simulated data: 1000 $(\\\\theta, x)$ pairs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ingredient1",
   "metadata": {},
   "source": [
    "### Ingredient 1: Dataset\n",
    "\n",
    "PyTorch provides `TensorDataset` and `DataLoader` to handle batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap data in a Dataset and DataLoader\n",
    "dataset = TensorDataset(theta, x)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Example: get one batch\n",
    "theta_batch, x_batch = next(iter(dataloader))\n",
    "print(f\"Batch shapes: theta={theta_batch.shape}, x={x_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ingredient2",
   "metadata": {},
   "source": [
    "### Ingredient 2: Neural Network\n",
    "\n",
    "We define a simple feedforward network using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple network: input (1D) -> hidden -> hidden -> output (1D)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(1, 32),   # input -> hidden\n",
    "    nn.ReLU(),          # activation\n",
    "    nn.Linear(32, 32),  # hidden -> hidden\n",
    "    nn.ReLU(),          # activation\n",
    "    nn.Linear(32, 1),   # hidden -> output\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ingredient3",
   "metadata": {},
   "source": [
    "### Ingredient 3: Loss Function\n",
    "\n",
    "For regression, we use **Mean Squared Error (MSE)**:\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum_i (\\hat{x}_i - x_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(prediction, target):\n",
    "    return torch.mean((prediction - target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ingredient4",
   "metadata": {},
   "source": [
    "### Ingredient 4: Optimizer\n",
    "\n",
    "The optimizer updates the network weights to minimize the loss. **Adam** is a good default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-optimizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ingredient5",
   "metadata": {},
   "source": [
    "### Ingredient 5: Training Loop\n",
    "\n",
    "The training loop ties everything together:\n",
    "```\n",
    "for each epoch:\n",
    "    for each batch:\n",
    "        1. Forward pass: compute predictions\n",
    "        2. Compute loss\n",
    "        3. Backward pass: compute gradients\n",
    "        4. Update weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: predict x from theta (the FORWARD direction)\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    for theta_batch, x_batch in dataloader:\n",
    "        optimizer.zero_grad()           # Reset gradients\n",
    "        prediction = net(theta_batch)   # Forward pass: theta -> x\n",
    "        loss = mse_loss(prediction, x_batch)  # Compare to true x\n",
    "        loss.backward()                 # Backward pass\n",
    "        optimizer.step()                # Update weights\n",
    "        losses.append(loss.item())\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what the network learned\n",
    "theta_test = torch.linspace(0, 1, 100).unsqueeze(1)\n",
    "with torch.no_grad():\n",
    "    x_pred = net(theta_test)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(theta, x, s=5, alpha=0.3, label=\"Data\")\n",
    "plt.plot(theta_test, x_pred, \"r-\", linewidth=2, label=\"Network prediction\")\n",
    "plt.xlabel(r\"$\\theta$ (parameter)\")\n",
    "plt.ylabel(r\"$x$ (observation)\")\n",
    "plt.title(r\"Regression works! Network learns $x = f(\\theta)$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"The network learned to predict the MEAN of x given theta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1-summary",
   "metadata": {},
   "source": [
    "### Part 1 Summary: The 5 Ingredients\n",
    "\n",
    "| Ingredient | What it does |\n",
    "|------------|-------------|\n",
    "| **Dataset** | Stores and batches your data |\n",
    "| **Network** | Maps inputs to outputs |\n",
    "| **Loss** | Measures prediction error |\n",
    "| **Optimizer** | Updates weights to reduce loss |\n",
    "| **Training loop** | Iterates until convergence |\n",
    "\n",
    "We trained a network to predict $x$ from $\\theta$ (the **forward** direction). This works well!\n",
    "\n",
    "But for **inference**, we need the opposite: given an observation $x$, what parameter $\\theta$ generated it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: The Inverse Problem\n",
    "\n",
    "Above, we used NNs in PyTorch to predict $x$ from $\\theta$. But for SBI, we need the **inverse**: given $x$, what is $\\theta$?\n",
    "\n",
    "Let's visualize what happens when we flip the axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-inverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(theta, x, s=5, alpha=0.3)\n",
    "plt.xlabel(r\"$\\theta$ (parameter)\")\n",
    "plt.ylabel(r\"$x$ (observation)\")\n",
    "plt.title(r\"Forward: Given $\\theta$, what is $x$?\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x, theta, s=5, alpha=0.3)\n",
    "plt.xlabel(r\"$x$ (observation)\")\n",
    "plt.ylabel(r\"$\\theta$ (parameter)\")\n",
    "plt.axvline(0.5, color=\"red\", linestyle=\"--\", linewidth=2, label=\"x = 0.5\")\n",
    "plt.title(r\"Inverse: Given $x$, what is $\\theta$?\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-think-first-1",
   "metadata": {},
   "source": [
    "### Think First!\n",
    "\n",
    "Look at the right plot above. Notice how the same data looks different when we swap the axes!\n",
    "\n",
    "- **Left (forward)**: Each $\\theta$ maps to roughly one $x$ value → well-defined function\n",
    "- **Right (inverse)**: Some $x$ values (like $x \\approx 0.5$) correspond to **multiple** $\\theta$ values!\n",
    "\n",
    "For $x = 0.5$, can you give a single \"best\" value for $\\theta$?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**No!** Multiple $\\theta$ values could have generated $x = 0.5$. The inverse problem is **ambiguous** - we need a **distribution** over $\\theta$, not a single point estimate.\n",
    "\n",
    "This is exactly what the **posterior** $p(\\theta|x)$ gives us!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-need-distribution",
   "metadata": {},
   "source": [
    "### We Need a Distribution!\n",
    "\n",
    "A simple regression network outputs one number. But we need a **distribution** over $\\theta$.\n",
    "\n",
    "**Key idea**: Instead of predicting a single value, predict the **parameters of a distribution**.\n",
    "\n",
    "For a Gaussian posterior approximation:\n",
    "$$q(\\theta|x) = \\mathcal{N}(\\theta; \\mu(x), \\sigma(x))$$\n",
    "\n",
    "The network takes $x$ as input and outputs $(\\mu, \\sigma)$ - the parameters that define the distribution.\n",
    "\n",
    "<img src=\"figures/nn.png\" width=\"700\">\n",
    "\n",
    "*The network outputs distribution parameters ($\\mu$, $\\sigma$), not a point estimate. The loss function encourages the predicted distribution to assign high probability to the true $\\theta$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-think-first-2",
   "metadata": {},
   "source": [
    "### Think First!\n",
    "\n",
    "If our network outputs $(\\mu, \\sigma)$ for a Gaussian, what should the **output dimension** be?\n",
    "\n",
    "- A) 1 (just the mean)\n",
    "- B) 2 (mean and standard deviation)\n",
    "- C) 3 (mean, standard deviation, and something else)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**B) 2** - We need both $\\mu$ and $\\sigma$ to fully specify a Gaussian distribution.\n",
    "\n",
    "In practice, we output $\\log(\\sigma^2)$ instead of $\\sigma$ for numerical stability (it can be any real number, while $\\sigma$ must be positive).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-nll-intro",
   "metadata": {},
   "source": [
    "### The Training Objective\n",
    "\n",
    "How do we train a network to output good distribution parameters?\n",
    "\n",
    "We want the predicted distribution $q(\\theta|x)$ to assign **high probability** to the true $\\theta$ values in our training data.\n",
    "\n",
    "**Loss function**: Negative Log-Likelihood (NLL)\n",
    "$$\\mathcal{L} = -\\frac{1}{N}\\sum_i \\log q(\\theta_i | x_i)$$\n",
    "\n",
    "Minimizing this is equivalent to **maximizing the probability** of the training data under our predicted distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercise-nll",
   "metadata": {},
   "source": [
    "### Exercise: Understanding the Loss\n",
    "\n",
    "The log-probability of $\\theta$ under a Gaussian $\\mathcal{N}(\\mu, \\sigma)$ is:\n",
    "$$\\log \\mathcal{N}(\\theta; \\mu, \\sigma) = -\\frac{1}{2}\\left(\\log(2\\pi) + \\log(\\sigma^2) + \\frac{(\\theta-\\mu)^2}{\\sigma^2}\\right)$$\n",
    "\n",
    "**Question**: What happens to the loss when...\n",
    "\n",
    "1. $\\mu$ is far from the true $\\theta$?\n",
    "2. $\\sigma$ is very small but $\\mu \\neq \\theta$?\n",
    "3. $\\sigma$ is very large?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "1. **$\\mu$ far from $\\theta$**: The $(\\theta-\\mu)^2$ term becomes large → loss increases → network learns to move $\\mu$ closer to $\\theta$\n",
    "\n",
    "2. **$\\sigma$ small but wrong $\\mu$**: Double penalty! Small $\\sigma^2$ in denominator makes $(\\theta-\\mu)^2/\\sigma^2$ huge → network learns it can't be overconfident when wrong\n",
    "\n",
    "3. **$\\sigma$ large**: The $\\log(\\sigma^2)$ term increases the loss → network learns not to be too uncertain (lazy)\n",
    "\n",
    "The loss balances accuracy ($\\mu$ close to $\\theta$) with calibrated uncertainty ($\\sigma$ matching true spread)!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-build-network",
   "metadata": {},
   "source": [
    "### Exercise: Building the Posterior Network\n",
    "\n",
    "Now it's your turn! Complete the network below that outputs distribution parameters.\n",
    "\n",
    "**Your task**: Fill in the `???` to make the output layer produce both $\\mu$ and $\\log\\sigma^2$ for each parameter dimension.\n",
    "\n",
    "**Hint**: If `theta_dim = 2`, how many outputs do you need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-density-net",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this class!\n",
    "\n",
    "class PosteriorNet(nn.Module):\n",
    "    \"\"\"Network that predicts posterior distribution parameters.\n",
    "    \n",
    "    Input: observation x (shape: batch_size x x_dim)\n",
    "    Output: parameters (mu, log_var) of Gaussian q(theta|x)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x_dim, theta_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(x_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, ???),  # TODO: What should the output dimension be?\n",
    "        )\n",
    "        self.theta_dim = theta_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns (mu, log_var) for the predicted posterior.\"\"\"\n",
    "        out = self.net(x)\n",
    "        mu = out[:, :self.theta_dim]\n",
    "        log_var = out[:, self.theta_dim:]  # log(sigma^2) for numerical stability\n",
    "        return ??, ??  # TODO: what to return here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oftajj0kjz",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Run the cell below to load the solution (or try completing it yourself first!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77iq5ag3zku",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solutions_nb_2b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2-summary",
   "metadata": {},
   "source": [
    "### Part 2 Summary\n",
    "\n",
    "- The inverse problem requires a **distribution**, not a point estimate\n",
    "- We use a neural network to predict **distribution parameters** $(\\mu, \\sigma)$\n",
    "- Training objective: **negative log-likelihood** - make the predicted distribution assign high probability to true $\\theta$ values\n",
    "\n",
    "This is the core idea of **Neural Posterior Estimation (NPE)**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part3-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Neural Posterior Estimation in Action\n",
    "\n",
    "Let's train our posterior network and see what it learns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train-npe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the posterior network\n",
    "posterior_net = PosteriorNet(x_dim=1, theta_dim=1)\n",
    "optimizer = Adam(posterior_net.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    for theta_batch, x_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute log probability of true theta under predicted posterior\n",
    "        log_prob = posterior_net.log_prob(theta_batch, x_batch)\n",
    "        \n",
    "        # Loss = negative log likelihood\n",
    "        loss = -log_prob.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "print(f\"Final NLL: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Negative Log-Likelihood\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-visualize-posteriors",
   "metadata": {},
   "source": [
    "### Visualizing the Learned Posteriors\n",
    "\n",
    "Let's see what posterior distributions the network predicts for different observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-posteriors",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "x_values = [0.3, 0.5, 0.8]\n",
    "theta_range = torch.linspace(0, 1, 200)\n",
    "\n",
    "for ax, x_obs in zip(axes, x_values):\n",
    "    # Get posterior parameters from network\n",
    "    x_tensor = torch.tensor([[x_obs]])\n",
    "    with torch.no_grad():\n",
    "        mu, log_var = posterior_net(x_tensor)\n",
    "        sigma = torch.exp(0.5 * log_var)\n",
    "    \n",
    "    # Plot the Gaussian posterior\n",
    "    posterior = torch.exp(-0.5 * ((theta_range - mu) / sigma)**2) / (sigma * math.sqrt(2 * math.pi))\n",
    "    ax.plot(theta_range, posterior.squeeze(), \"b-\", linewidth=2)\n",
    "    ax.fill_between(theta_range, 0, posterior.squeeze(), alpha=0.3)\n",
    "    ax.axvline(mu.item(), color=\"red\", linestyle=\"--\", label=f\"$\\\\mu$={mu.item():.2f}\")\n",
    "    ax.set_xlabel(r\"$\\theta$\")\n",
    "    ax.set_ylabel(r\"$q(\\theta|x)$\")\n",
    "    ax.set_title(f\"Posterior for x = {x_obs}\")\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-overlay-posteriors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay posteriors on the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, theta, s=5, alpha=0.2, label=\"Training data\")\n",
    "\n",
    "# Show posteriors for several x values\n",
    "for x_obs in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    x_tensor = torch.tensor([[x_obs]])\n",
    "    with torch.no_grad():\n",
    "        mu, log_var = posterior_net(x_tensor)\n",
    "        sigma = torch.exp(0.5 * log_var)\n",
    "    \n",
    "    # Plot posterior as horizontal Gaussian\n",
    "    theta_range = torch.linspace(0, 1, 100)\n",
    "    posterior = torch.exp(-0.5 * ((theta_range - mu) / sigma)**2)\n",
    "    plt.plot(x_obs + posterior.squeeze() * 0.1, theta_range, \"r-\", linewidth=1.5)\n",
    "\n",
    "plt.xlabel(r\"$x$ (observation)\")\n",
    "plt.ylabel(r\"$\\theta$ (parameter)\")\n",
    "plt.title(r\"NPE: Learned Posterior $q(\\theta|x)$\")\n",
    "plt.plot([], [], \"r-\", linewidth=2, label=\"Predicted posteriors\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Red curves show the posterior distribution for different observations.\")\n",
    "print(\"The network learned to output appropriate (mu, sigma) for each x!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-quick-check",
   "metadata": {},
   "source": [
    "### Quick Check: What Did the Network Learn?\n",
    "\n",
    "Let's verify that the network outputs different distribution parameters for different inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-check-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network outputs for different observations:\")\n",
    "print(\"-\" * 40)\n",
    "for x_obs in [0.2, 0.5, 0.8, 1.1]:\n",
    "    x_tensor = torch.tensor([[x_obs]])\n",
    "    with torch.no_grad():\n",
    "        mu, log_var = posterior_net(x_tensor)\n",
    "        sigma = torch.exp(0.5 * log_var)\n",
    "    print(f\"x = {x_obs:.1f}  →  mu = {mu.item():.3f}, sigma = {sigma.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-amortization",
   "metadata": {},
   "source": [
    "### The Power of Amortization\n",
    "\n",
    "Notice something important: we can get posteriors for **any** observation instantly!\n",
    "\n",
    "This is called **amortization** - we pay the cost of training once, then inference is instant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-limitations",
   "metadata": {},
   "source": [
    "### Limitations of Gaussian Posteriors\n",
    "\n",
    "Our simple Gaussian approximation has limitations:\n",
    "\n",
    "1. **Can't capture multi-modality**: What if two different $\\theta$ values could equally explain an observation? A single Gaussian can't represent this.\n",
    "\n",
    "2. **Limited shape**: Real posteriors can be skewed, heavy-tailed, or have complex correlations.\n",
    "\n",
    "**Solutions** (covered in Session 3):\n",
    "- **Mixture Density Networks**: Output parameters for multiple Gaussians\n",
    "- **Normalizing Flows**: Flexible distributions that can model any shape\n",
    "- **The `sbi` package**: Implements these advanced methods with a simple API!\n",
    "\n",
    "<img src=\"figures/npe_illustration.png\" width=\"600\">\n",
    "\n",
    "*A Mixture Density Network outputs mixing coefficients ($\\alpha$), means ($\\mu$), and standard deviations ($\\sigma$) for multiple Gaussian components, allowing it to represent multi-modal posteriors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### The NPE Recipe\n",
    "\n",
    "1. **Generate training data**: Sample $\\theta \\sim p(\\theta)$, simulate $x \\sim p(x|\\theta)$\n",
    "2. **Train a neural network**: Learn to predict distribution parameters from $x$\n",
    "3. **Loss function**: Negative log-likelihood $-\\log q_\\phi(\\theta|x)$\n",
    "4. **At inference**: Just run the network on your observation!\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "**Posterior approximation** (Gaussian):\n",
    "$$q_\\phi(\\theta|x) = \\mathcal{N}(\\theta; \\mu_\\phi(x), \\sigma_\\phi(x))$$\n",
    "\n",
    "**Training objective**:\n",
    "$$\\mathcal{L} = -\\mathbb{E}_{p(\\theta, x)}[\\log q_\\phi(\\theta|x)]$$\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Session 3**, we'll:\n",
    "- Apply NPE to the Lotka-Volterra model from Session 2\n",
    "- Use the `sbi` package with flexible normalizing flow posteriors\n",
    "- Learn about the full SBI workflow including validation and diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-learning-goals",
   "metadata": {},
   "source": [
    "## Learning Goals Checklist\n",
    "\n",
    "After this notebook, you should be able to:\n",
    "\n",
    "- [ ] Name the 5 key ingredients of PyTorch training\n",
    "- [ ] Explain why the inverse problem requires a distribution, not a point estimate\n",
    "- [ ] Describe what a posterior network outputs (distribution parameters)\n",
    "- [ ] Explain the negative log-likelihood loss and what it encourages\n",
    "- [ ] Define \"amortization\" and why it makes NPE efficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SBI Hackathon)",
   "language": "python",
   "name": "sbi-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
