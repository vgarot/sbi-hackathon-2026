{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Introduction to SBI\n",
    "\n",
    "In this notebook, we will implement and apply the rejection ABC algorithm to the Lotka-Volterra (LV) predator prey model. \n",
    "\n",
    "We have implemented a set of helper functions for running LV simulations and calculating summary statistics. \n",
    "\n",
    "Your central exercise will be to implement the rejection ABC algorithm yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import trange\n",
    "\n",
    "# Helper functions\n",
    "from simulators import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    lotka_volterra_simulator,\n",
    "    simulate,\n",
    ")\n",
    "from utils import corner_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement rejection ABC for the Lotka-Volterra model\n",
    "\n",
    "The Lotka-Volterra model describes predator-prey dynamics (e.g., wolves and deer):\n",
    "\n",
    "- $\\alpha$: prey birth rate\n",
    "- $\\beta$: predation rate  \n",
    "- $\\gamma$: predator death rate\n",
    "- $\\delta$: predator reproduction rate per prey consumed\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d(\\text{prey})}{dt} &= \\alpha \\cdot \\text{prey} - \\beta \\cdot \\text{prey} \\cdot \\text{predator} \\\\\n",
    "\\frac{d(\\text{predator})}{dt} &= \\delta \\cdot \\text{prey} \\cdot \\text{predator} - \\gamma \\cdot \\text{predator}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Simulator\n",
    "\n",
    "Before we start inference, let's understand the simulator by visualizing its outputs:\n",
    "1. **Time series dynamics**: How do populations evolve over time?\n",
    "2. **Effect of parameters**: How do different parameters affect the dynamics?\n",
    "3. **Summary statistics**: What features of the time series do we use for inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up prior and simulator\n",
    "prior = create_lotka_volterra_prior()\n",
    "x_o, theta_o = generate_observed_data(use_autocorrelation=True)\n",
    "\n",
    "# Sample parameters from prior for visualization\n",
    "n_simulations = 100\n",
    "theta_prior = prior.sample((n_simulations,))\n",
    "\n",
    "# Simulate time series for prior samples and ground truth\n",
    "time = np.arange(0, 200, 0.1)\n",
    "ts_observed = simulate(theta_o.numpy())\n",
    "ts_prior = [simulate(theta_prior[i].numpy()) for i in range(n_simulations)]\n",
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Prey population\n",
    "ax = axes[0]\n",
    "for ts in ts_prior:\n",
    "    ax.plot(time, ts[:, 0], color=\"C0\", alpha=0.1, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 0], color=\"k\", linewidth=2, label=r\"observed ($\\theta_o$)\")\n",
    "ax.plot([], [], color=\"C0\", alpha=0.5, label=\"prior samples\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"Prey (Deer) Population Dynamics\")\n",
    "ax.legend()\n",
    "\n",
    "# Predator population\n",
    "ax = axes[1]\n",
    "for ts in ts_prior:\n",
    "    ax.plot(time, ts[:, 1], color=\"C0\", alpha=0.1, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 1], color=\"k\", linewidth=2, label=r\"observed ($\\theta_o$)\")\n",
    "ax.plot([], [], color=\"C0\", alpha=0.5, label=\"prior samples\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"Predator (Wolf) Population Dynamics\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ground truth parameters: Î±={theta_o[0]:.3f}, Î²={theta_o[1]:.3f}, Î´={theta_o[2]:.3f}, Î³={theta_o[3]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "Raw time series are high-dimensional (2000 time points Ã— 2 populations). For ABC, we compress them into **summary statistics**:\n",
    "\n",
    "**Moments (5 per population):**\n",
    "- **Mean**: Average population level\n",
    "- **Std**: Variability in population\n",
    "- **Max**: Peak population reached\n",
    "- **Skewness**: Asymmetry of the distribution\n",
    "- **Kurtosis**: \"Tailedness\" of the distribution\n",
    "\n",
    "**Autocorrelation (5 per population):**\n",
    "- Captures temporal structure and periodicity of oscillations\n",
    "- Measured at lags corresponding to 1, 5, 10, 20, and 40 days\n",
    "\n",
    "These 10 statistics per population give us a **20-dimensional summary vector** $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for prior samples (with autocorrelation)\n",
    "x_prior = lotka_volterra_simulator(theta_prior, use_autocorrelation=True)\n",
    "\n",
    "print(f\"Summary statistics shape: {x_prior.shape}\")  # Should be (100, 20)\n",
    "print(f\"Observed summary statistics shape: {x_o.shape}\")\n",
    "\n",
    "# Visualize summary statistics distribution\n",
    "stat_names = [\"Mean\", \"Std\", \"Max\", \"Skew\", \"Kurt\", \"AC1\", \"AC5\", \"AC10\", \"AC20\", \"AC40\"]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Deer statistics\n",
    "ax = axes[0]\n",
    "positions = np.arange(10)\n",
    "bp = ax.boxplot(x_prior[:, :10].numpy(), positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C0')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_o.squeeze()[:10].numpy(), color=\"k\", s=100, zorder=10, marker=\"x\", label=r\"observed ($x_o$)\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names, rotation=45, ha='right')\n",
    "ax.set_title(\"Deer Summary Statistics (Prior Samples)\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()\n",
    "\n",
    "# Wolf statistics\n",
    "ax = axes[1]\n",
    "bp = ax.boxplot(x_prior[:, 10:20].numpy(), positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C0')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_o.squeeze()[10:20].numpy(), color=\"k\", s=100, zorder=10, marker=\"x\", label=r\"observed ($x_o$)\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names, rotation=45, ha='right')\n",
    "ax.set_title(\"Wolf Summary Statistics (Prior Samples)\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Implement ABC!\n",
    "\n",
    "We've explored the simulator and understand our summary statistics. Now it's time to implement rejection ABC to infer the parameters Î¸ from observed data x_o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for ABC (provided)\n",
    "\n",
    "def distance(x: Tensor, x_o: Tensor) -> Tensor:\n",
    "    \"\"\"Compute mean squared error between simulated and observed summary statistics.\"\"\"\n",
    "    return torch.mean((x - x_o) ** 2, dim=-1)\n",
    "\n",
    "def sample_and_simulate(num_samples: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Sample parameters from prior and simulate summary statistics.\"\"\"\n",
    "    theta = prior.sample((num_samples,))\n",
    "    x = lotka_volterra_simulator(theta, use_autocorrelation=True)\n",
    "    return theta, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think First! ðŸ¤”\n",
    "\n",
    "Before we implement ABC, consider these questions:\n",
    "\n",
    "1. **What happens as Îµ â†’ 0?** (very small threshold)\n",
    "2. **What happens as Îµ â†’ âˆž?** (very large threshold)\n",
    "3. **What's the trade-off we're making with Îµ?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "1. **Îµ â†’ 0**: Only exact matches accepted â†’ true posterior, but acceptance rate â†’ 0\n",
    "2. **Îµ â†’ âˆž**: Everything accepted â†’ posterior = prior (no information gained)\n",
    "3. **Trade-off**: Accuracy vs. computational efficiency (acceptance rate)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection ABC Algorithm\n",
    "\n",
    "**Goal**: Approximate the posterior $p(\\theta | x_o)$ by accepting parameters whose simulations are \"close\" to the observation.\n",
    "\n",
    "**Algorithm**:\n",
    "```\n",
    "for i = 1 to N:\n",
    "    1. Sample Î¸ from prior: Î¸ ~ p(Î¸)\n",
    "    2. Simulate data: x ~ p(x | Î¸)  \n",
    "    3. If distance(x, x_o) < Îµ: accept Î¸\n",
    "```\n",
    "\n",
    "**Key choices**:\n",
    "- **Distance function** $d(x, x_o)$: How to measure similarity (e.g., MSE on summary stats)\n",
    "- **Threshold** $\\varepsilon$: Trade-off between accuracy and acceptance rate\n",
    "\n",
    "As $\\varepsilon \\to 0$, the ABC posterior converges to the true posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Implement Rejection ABC\n",
    "\n",
    "Implement the rejection ABC algorithm:\n",
    "1. Loop through simulation budget\n",
    "2. Sample Î¸ from prior, simulate x\n",
    "3. Accept Î¸ if distance(x, x_o) < Îµ\n",
    "4. Return accepted samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task: Implement the rejection ABC loop\n",
    "\n",
    "Complete the function below. The key logic is:\n",
    "- Sample Î¸ from prior\n",
    "- Simulate summary statistics x\n",
    "- Accept Î¸ if it produces x \"close enough\" to x_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_abc(num_simulations: int, epsilon: float) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Run rejection ABC and return accepted posterior samples.\n",
    "    \n",
    "    Args:\n",
    "        num_simulations: Total number of simulations to run\n",
    "        epsilon: Acceptance threshold for distance\n",
    "        \n",
    "    Returns:\n",
    "        posterior_samples: Accepted Î¸ values\n",
    "        all_theta: All sampled Î¸ values (for visualization)\n",
    "    \"\"\"\n",
    "    accepted = []\n",
    "    all_theta = []\n",
    "    \n",
    "    for _ in trange(num_simulations):\n",
    "        # Step 1: Sample from prior and simulate\n",
    "        # Make sure to use lotka_volterra_simulator(theta, use_autocorrelation=True)\n",
    "        \n",
    "        # Step 2: Accept if close enough to observation\n",
    "        # TODO: Your code here - when should we accept theta?\n",
    "        pass\n",
    "    \n",
    "    return torch.cat(accepted), torch.cat(all_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solutions_nb_2a_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ABC with different Îµ values\n",
    "\n",
    "Now run your implementation! Start with Îµ=5 and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rejection ABC\n",
    "# Note: This takes ~1 minute for 10,000 simulations\n",
    "\n",
    "# # YOUR CODE HERE: set epsilon and run your function.\n",
    "eps = ...\n",
    "num_simulations = 5000\n",
    "\n",
    "posterior_samples, theta = rejection_abc(num_simulations, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Check: Acceptance Rate\n",
    "\n",
    "What fraction of your simulations led to accepted parameter sets? This tells us how \"efficient\" ABC is for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What fraction of simulations were accepted?\n",
    "\n",
    "# YOUR CODE HERE (hint: use len() and the variables from above)\n",
    "acceptance_rate = \n",
    "\n",
    "print(f\"Acceptance rate: {acceptance_rate:.1%}\")\n",
    "print(f\"This means {100 - 100*acceptance_rate:.0f}% of simulations were 'wasted'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think First! ðŸ¤”\n",
    "\n",
    "Before we move on and try out many different epsilons: \n",
    "\n",
    "**Can you think of a less wasteful way of implementing the rejection ABC function?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "Let's change the signature to take pre-simulated `theta` and `x` so that we can re-use simulations.\n",
    "\n",
    "```python\n",
    "def rejection_abc_smart(theta: Tensor, x: Tensor, epsilon: float) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Run rejection ABC and return accepted posterior samples.\"\"\"\n",
    "    accepted = []\n",
    "\n",
    "    for theta_i, x_i in zip(theta, x):\n",
    "        if distance(x_i, x_o) < epsilon:\n",
    "            accepted.append(theta_i)\n",
    "\n",
    "    if not len(accepted):\n",
    "        raise ValueError(\"No parameters were accepted, epsilon likely too small\")\n",
    "    \n",
    "    return torch.stack(accepted)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solutions_nb_2a_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Finding a good Îµ value\n",
    "\n",
    "In practice we need find a specific Îµ value, given a certain simulation budget or desired number of posterior samples. \n",
    "\n",
    "Let's say we have a budget of **20_000** simulations, which epsilon should we choose to obtain **200** posterior samples? \n",
    "\n",
    "### Your task\n",
    "\n",
    "- Simulate 20_000 LV traces and save them in `theta` and `x`.\n",
    "- Using the `rejection_abc_smart` function (copy it from the solution cell above), find an Îµ such that we obtain around 200 posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-simulate\n",
    "\n",
    "num_simulations = 20_000\n",
    "\n",
    "# TODO: your could goes here\n",
    "theta = []\n",
    "x = []\n",
    "\n",
    "# loop with progress bar.\n",
    "for _ in trange(num_simulations):\n",
    "    # theta_i = ...\n",
    "    # x_i = ...\n",
    "    \n",
    "    theta.append(theta_i)\n",
    "    x.append(x_i)\n",
    "\n",
    "# Collect lists into tensors.\n",
    "theta = torch.cat(theta)\n",
    "x = torch.cat(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different epsilon values\n",
    "\n",
    "# TODO: fill in epsilon values to try here.\n",
    "# Hint: use values between 20 and 0.1\n",
    "epsilon_values = []\n",
    "results = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    # Note: here we just re-use our simulations from above.\n",
    "    samples = rejection_abc_smart(theta, x, eps)\n",
    "    results[eps] = samples\n",
    "    \n",
    "# Plot comparison\n",
    "param_idx = 2  # let's visualize the delta param \n",
    "fig, axes = plt.subplots(1, len(epsilon_values), figsize=(15, 3))\n",
    "for ax, eps in zip(axes, epsilon_values):\n",
    "    ax.hist(results[eps][:, param_idx], bins=20, alpha=0.7, density=True)\n",
    "    ax.axvline(theta_o[param_idx].item(), color=\"k\", linestyle=\"--\")\n",
    "    ax.set_title(f\"Îµ = {eps}\\n({len(results[eps])} samples)\")\n",
    "    ax.set_xlabel(r\"$\\delta$\")\n",
    "    ax.set_xlim([0.005, 0.03])\n",
    "plt.suptitle(r\"Effect of Îµ on posterior for $\\delta$ (predator birth rate)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final step: Select your epsilon and get posterior samples\n",
    "\n",
    "final_epsilon = ???\n",
    "posterior_samples = rejection_abc_smart(theta, x, final_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing multi-dimensional posteriors\n",
    "\n",
    "Above, we plotted the histograms for a single parameter. How can we visualize the full four-dimensional posterior? \n",
    "\n",
    "We cannot! \n",
    "\n",
    "Instead, below we generate a so-colled corner plot or \"pairplot\". It it shows: \n",
    "\n",
    "- on the diagonal: the one-dimensional marginal samples of each parameter\n",
    "- on the off-diagonal: each pair of two-dimensional marginals posterior samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import corner_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"]\n",
    "\n",
    "corner_plot([theta[:2000], posterior_samples], # pass a list of samples to plot.\n",
    "            labels=[\"prior\", \"posterior\"], \n",
    "            theta_true=theta_o, \n",
    "            param_names=param_names,\n",
    "           );\n",
    "plt.suptitle(\"Prior vs. Posteriors Samples\", y=1.02, fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is this approximation? *The Posterior Predictive Check*\n",
    "\n",
    "A key validation step in SBI: do simulations from posterior samples produce data similar to the observation?\n",
    "\n",
    "We'll compare:\n",
    "1. **Time series**: Population dynamics from posterior samples vs. ground truth\n",
    "2. **Summary statistics**: Distribution of summary stats from posterior vs. observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check: simulate from posterior samples and compare with observation\n",
    "\n",
    "# Number of posterior samples to use for predictive check\n",
    "n_predictive = min(50, len(posterior_samples))\n",
    "posterior_subset = posterior_samples[:n_predictive]\n",
    "\n",
    "# Simulate time series for ground truth and posterior samples\n",
    "time = np.arange(0, 200, 0.1)  # Same as in simulate()\n",
    "ts_observed = simulate(theta_o.numpy())\n",
    "ts_posterior = [simulate(posterior_subset[i].numpy()) for i in range(n_predictive)]\n",
    "\n",
    "# Simulate summary statistics for posterior samples (with autocorrelation)\n",
    "x_posterior = lotka_volterra_simulator(posterior_subset, use_autocorrelation=True)\n",
    "\n",
    "# Create figure with 2 rows: time series (top), summary statistics (bottom)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# --- Top row: Time series ---\n",
    "# Left: Prey (Deer) population\n",
    "ax = axes[0, 0]\n",
    "for ts in ts_posterior:\n",
    "    ax.plot(time, ts[:, 0], color=\"C1\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 0], color=\"k\", linewidth=2, label=\"observed (ground truth)\")\n",
    "ax.plot([], [], color=\"C1\", alpha=0.5, label=\"posterior predictive\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"Prey (Deer) Population\")\n",
    "ax.legend()\n",
    "\n",
    "# Right: Predator (Wolf) population\n",
    "ax = axes[0, 1]\n",
    "for ts in ts_posterior:\n",
    "    ax.plot(time, ts[:, 1], color=\"C1\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 1], color=\"k\", linewidth=2, label=\"observed (ground truth)\")\n",
    "ax.plot([], [], color=\"C1\", alpha=0.5, label=\"posterior predictive\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"Predator (Wolf) Population\")\n",
    "ax.legend()\n",
    "\n",
    "# --- Bottom row: Summary statistics ---\n",
    "stat_names = [\"Mean\", \"Std\", \"Max\", \"Skew\", \"Kurt\", \"AC1\", \"AC5\", \"AC10\", \"AC20\", \"AC40\"]\n",
    "x_obs = x_o.squeeze().numpy()\n",
    "\n",
    "# Left: Deer summary statistics\n",
    "ax = axes[1, 0]\n",
    "deer_stats_posterior = x_posterior[:, :10].numpy()\n",
    "positions = np.arange(10)\n",
    "bp = ax.boxplot(deer_stats_posterior, positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C1')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_obs[:10], color=\"k\", s=100, zorder=10, marker=\"x\", label=\"observed\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names, rotation=45, ha='right')\n",
    "ax.set_title(\"Deer Summary Statistics\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()\n",
    "\n",
    "# Right: Wolf summary statistics\n",
    "ax = axes[1, 1]\n",
    "wolf_stats_posterior = x_posterior[:, 10:20].numpy()\n",
    "bp = ax.boxplot(wolf_stats_posterior, positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C1')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_obs[10:20], color=\"k\", s=100, zorder=10, marker=\"x\", label=\"observed\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names, rotation=45, ha='right')\n",
    "ax.set_title(\"Wolf Summary Statistics\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- finding a good trade-off between simulation budget and number of accepted posterior samples is difficult!\n",
    "- posterior predictive check: our simulations from the posterior center on the observed summary statistics, but the corresponding time-series show a lot of variance\n",
    "- we would need more simulations and a smaller epsilon! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of ABC\n",
    "\n",
    "- Conceptually simple: fast to implement and apply\n",
    "- No neural network training required\n",
    "- Works with any simulator that can generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of ABC\n",
    "\n",
    "- Curse of dimensionality: struggles with high-dimensional $\\theta$ or $x$\n",
    "- Ad-hoc choices needed: distance metric, rejection threshold $\\epsilon$, summary statistics\n",
    "- Many simulations \"wasted\" on rejected samples\n",
    "- Not amortized: must re-run for each new observation $x_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "At this point you should understand:\n",
    "\n",
    "- What is a simulation-based model / simulator?\n",
    "- Why do we need to infer parameters?\n",
    "- What is Bayesian inference, and why is it better than point estimates?\n",
    "- Why can't we just use MCMC when we have a simulator?\n",
    "- What is simulation-based inference?\n",
    "- How does rejection ABC work?\n",
    "- What are the benefits and downsides of rejection ABC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Your Problem ðŸ”—\n",
    "\n",
    "Now think about your own simulator:\n",
    "\n",
    "| Question | Consider |\n",
    "|----------|----------|\n",
    "| **Parameter dimension** | ABC struggles with dim(Î¸) > 10. How many parameters do you have? |\n",
    "| **Simulation cost** | ABC needs many simulations. How long does one run of your simulator take? |\n",
    "| **Summary statistics** | What features of your output would you use? Are they sufficient? |\n",
    "| **Distance metric** | Is MSE appropriate, or do you need domain-specific distances? |\n",
    "\n",
    "**Rule of thumb**: ABC is a good starting point if:\n",
    "- Î¸ is low-dimensional (< 10 parameters)\n",
    "- Simulator runs in < 1 second  \n",
    "- You have good intuition for summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "Feel free to ask about any concepts covered in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Neural Posterior Estimation (NPE)\n",
    "\n",
    "Use *conditional density estimation* to learn a parametric approximation to the posterior distribution.\n",
    "\n",
    "Instead of rejection sampling, train a neural network $q_\\phi(\\theta|x)$ to directly approximate $p(\\theta|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "**Continue to the next notebook** to learn about conditional density estimation and neural posterior estimation (NPE)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (SBI Hackathon)",
   "language": "python",
   "name": "sbi-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
