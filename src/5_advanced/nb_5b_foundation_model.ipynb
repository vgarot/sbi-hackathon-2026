{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d30650e",
   "metadata": {},
   "source": [
    "# Foundation Models for SBI: NPE-PFN\n",
    "\n",
    "**Time: ~15 minutes**\n",
    "\n",
    "In the previous notebooks, we:\n",
    "1. **Notebook 3**: Learned the `sbi` workflow for NPE (training normalizing flows)\n",
    "2. **Notebook 4**: Explored different summary statistics\n",
    "3. **Notebook 5**: Learned how to diagnose our posteriors\n",
    "\n",
    "In all these approaches, we had to **train** a neural network on our simulated data. What if we could skip training entirely?\n",
    "\n",
    "> **Foundation Models for SBI**: Pre-trained models that work \"out of the box\" on new problems!\n",
    "\n",
    "## What We'll Learn\n",
    "\n",
    "1. **NPE-PFN**: A foundation model for SBI based on Prior-data Fitted Networks (TabPFN)\n",
    "2. **Amortized inference without training**: Just provide (θ, x) pairs and get a posterior!\n",
    "3. **TSNPE-PFN**: Sequential version that focuses simulations on a specific observation\n",
    "\n",
    "**Paper**: \n",
    "- NPE-PFN: [Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models](https://arxiv.org/abs/2504.17660)\n",
    "- TabPFN: [Accurate predictions on small data with a tabular foundation model](https://www.nature.com/articles/s41586-024-08328-6)\n",
    "\n",
    "**Repository**: [github.com/mackelab/npe-pfn](https://github.com/mackelab/npe-pfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f8769",
   "metadata": {},
   "source": [
    "---\n",
    "## Installation\n",
    "\n",
    "The `npe-pfn` package needs to be cloned from GitHub. **Run the cell below once** to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install npe-pfn (run once)\n",
    "!git clone https://github.com/mackelab/npe-pfn.git 2>/dev/null || echo \"Repository already exists\"\n",
    "!uv pip install -e npe-pfn -q\n",
    "\n",
    "# Refresh Python's import system to detect the newly installed package\n",
    "import importlib\n",
    "import site\n",
    "importlib.invalidate_caches()\n",
    "site.main()  # Reload site-packages\n",
    "\n",
    "import npe_pfn\n",
    "print(\"npe-pfn installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e0e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sbi.inference import NPE\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "from npe_pfn import TabPFN_Based_NPE_PFN, run_tsnpe_pfn\n",
    "\n",
    "from simulators import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    lotka_volterra_simulator,\n",
    "    simulate,\n",
    ")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# set device\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c61ec",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Same Lotka-Volterra Problem\n",
    "\n",
    "We continue with the predator-prey model from previous notebooks. The goal is to infer the 4 Lotka-Volterra parameters from summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup prior and observed data\n",
    "prior = create_lotka_volterra_prior()\n",
    "x_o, theta_o = generate_observed_data(use_autocorrelation=True)\n",
    "\n",
    "# For visualization later\n",
    "time = np.arange(0, 200, 0.1)\n",
    "ts_observed = simulate(theta_o.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360f153",
   "metadata": {},
   "source": [
    "---\n",
    "## Think First!\n",
    "\n",
    "Before we use NPE-PFN, let's understand the key concepts:\n",
    "\n",
    "**Question 1**: Standard NPE requires training a neural network for each new problem. What are the advantages and disadvantages of this?\n",
    "\n",
    "**Question 2**: How can a pre-trained model work on problems it has never seen before?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "1. **Standard NPE trade-offs:**\n",
    "   - **Advantages**: Tailored to your specific problem, can handle complex posteriors\n",
    "   - **Disadvantages**: Requires training time, need many simulations, hyperparameter tuning\n",
    "\n",
    "2. **How NPE-PFN generalizes:**\n",
    "   - Based on TabPFN, a transformer trained on synthetic tabular regression problems\n",
    "   - Learns \"how to do regression\" rather than a specific regression task\n",
    "   - At test time: Conditions on your (θ, x) pairs as context → outputs posterior!\n",
    "   - This is **in-context learning**: The model learns from examples you provide\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adcf77",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Standard NPE (Baseline) -- No Tasks\n",
    "\n",
    "First, let's run standard NPE as a baseline for comparison. \n",
    "\n",
    "NOTE: we only consider 100 simulations here for now -- that is extremely little! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "num_simulations = 100\n",
    "\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = lotka_volterra_simulator(theta, use_autocorrelation=True)\n",
    "\n",
    "print(f\"Generated {num_simulations} simulations\")\n",
    "print(f\"theta shape: {theta.shape}, x shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train standard NPE\n",
    "print(\"Training standard NPE...\")\n",
    "npe = NPE(prior)\n",
    "npe.append_simulations(theta, x).train()\n",
    "\n",
    "posterior_npe = npe.build_posterior()\n",
    "samples_npe = posterior_npe.sample((10_000,), x=x_o)\n",
    "\n",
    "print(f\"\\nStandard NPE trained! Posterior samples shape: {samples_npe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beddabe",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: NPE-PFN (Foundation Model)\n",
    "\n",
    "Now let's use NPE-PFN — a foundation model that requires **no training**!\n",
    "\n",
    "### How NPE-PFN Works\n",
    "\n",
    "NPE-PFN is based on [TabPFN](https://arxiv.org/abs/2207.01848), a transformer that was pre-trained to solve tabular prediction problems. The key insight:\n",
    "\n",
    "1. **Pre-training**: TabPFN was trained on millions of synthetic regression problems\n",
    "2. **In-context learning**: At test time, it takes your (θ, x) pairs as \"context\"\n",
    "3. **Posterior prediction**: Given a new observation x_o, it predicts the posterior over θ\n",
    "\n",
    "**No gradient updates needed** — just forward passes through the pre-trained network!\n",
    "\n",
    "**Note**: The default context size is 10,000 simulations. If you provide more, NPE-PFN will filter them based on Euclidean distance to x_o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997339a6",
   "metadata": {},
   "source": [
    "### Your Task: Run NPE-PFN\n",
    "\n",
    "Complete the code below to run inference with NPE-PFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c53225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPE-PFN inference\n",
    "print(\"Running NPE-PFN (no training needed!)...\")\n",
    "\n",
    "# TODO: Create NPE-PFN posterior and append simulations\n",
    "# npe_pfn_posterior = ...\n",
    "\n",
    "# TODO: Sample from the posterior (use 10_000 samples)\n",
    "# samples_pfn = ...\n",
    "\n",
    "print(f\"\\nNPE-PFN done! Posterior samples shape: {samples_pfn.shape}\")\n",
    "print(\"Notice: No training step was needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bdfb8d",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: TSNPE-PFN (Sequential Version)\n",
    "\n",
    "Standard NPE-PFN uses simulations from the prior. But what if we want to focus on a specific observation?\n",
    "\n",
    "**TSNPE-PFN** (Truncated Sequential NPE-PFN) is a sequential variant that:\n",
    "1. Starts with prior samples\n",
    "2. Identifies which prior regions are consistent with x_o\n",
    "3. Focuses new simulations on promising regions\n",
    "4. Iterates to refine the posterior\n",
    "\n",
    "This is similar to SNPE (Sequential NPE) but using the foundation model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344b468",
   "metadata": {},
   "source": [
    "### Your Task: Run TSNPE-PFN\n",
    "\n",
    "Complete the code below to run sequential inference with TSNPE-PFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNPE-PFN sequential inference\n",
    "print(\"Running TSNPE-PFN (sequential, focused on x_o)...\\n\")\n",
    "\n",
    "# Define simulator wrapper for TSNPE-PFN\n",
    "def simulator(theta):\n",
    "    return lotka_volterra_simulator(theta, use_autocorrelation=True)\n",
    "\n",
    "# TODO: Run TSNPE-PFN\n",
    "# Hint: Use run_tsnpe_pfn() with the prior, simulator, and observation\n",
    "# tsnpe_pfn_posterior = ...\n",
    "# samples_tsnpepfn = ...\n",
    "\n",
    "\n",
    "print(f\"\\nTSNPE-PFN done! Posterior samples shape: {samples_tsnpepfn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c01d5",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing All Methods\n",
    "\n",
    "Now let's compare the posteriors from all three methods:\n",
    "1. **Standard NPE**: Trained normalizing flow\n",
    "2. **NPE-PFN**: Foundation model (no training)\n",
    "3. **TSNPE-PFN**: Sequential foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25512a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare posteriors with pairplot\n",
    "param_labels = [r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"]\n",
    "limits = [[0.05, 0.15], [0.01, 0.03], [0.005, 0.03], [0.005, 0.15]]\n",
    "\n",
    "fig, axes = pairplot(\n",
    "    [samples_npe, samples_pfn, samples_tsnpepfn],\n",
    "    limits=limits,\n",
    "    labels=param_labels,\n",
    "    figsize=(10, 10),\n",
    "    points=theta_o,\n",
    "    points_colors=\"red\",\n",
    "    diag=\"kde\",\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "fig.legend(\n",
    "    [\"Standard NPE\", \"NPE-PFN\", \"TSNPE-PFN\"],\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(0.95, 0.95),\n",
    ")\n",
    "plt.suptitle(\"Posterior Comparison: Standard NPE vs Foundation Models\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior Predictive Check for all methods\n",
    "def plot_posterior_predictive_comparison(samples_list, labels, theta_o, ts_observed, time, n_samples=30):\n",
    "    \"\"\"Compare posterior predictive simulations for multiple methods.\"\"\"\n",
    "    fig, axes = plt.subplots(len(samples_list), 2, figsize=(14, 4*len(samples_list)))\n",
    "\n",
    "    colors = [\"C0\", \"C1\", \"C2\"]\n",
    "\n",
    "    for row, (samples, label, color) in enumerate(zip(samples_list, labels, colors)):\n",
    "        indices = np.random.choice(len(samples), size=n_samples, replace=False)\n",
    "\n",
    "        for idx in indices:\n",
    "            theta_sample = samples[idx].numpy()\n",
    "            ts_sample = simulate(theta_sample)\n",
    "            axes[row, 0].plot(time, ts_sample[:, 0], color=color, alpha=0.2, linewidth=0.5)\n",
    "            axes[row, 1].plot(time, ts_sample[:, 1], color=color, alpha=0.2, linewidth=0.5)\n",
    "\n",
    "        # Plot ground truth\n",
    "        axes[row, 0].plot(time, ts_observed[:, 0], color=\"black\", linewidth=2, label=\"Observed\")\n",
    "        axes[row, 1].plot(time, ts_observed[:, 1], color=\"black\", linewidth=2, label=\"Observed\")\n",
    "\n",
    "        axes[row, 0].set_ylabel(\"Population\")\n",
    "        axes[row, 0].set_title(f\"Prey - {label}\")\n",
    "        axes[row, 0].legend()\n",
    "\n",
    "        axes[row, 1].set_title(f\"Predator - {label}\")\n",
    "        axes[row, 1].legend()\n",
    "\n",
    "    axes[-1, 0].set_xlabel(\"Time (days)\")\n",
    "    axes[-1, 1].set_xlabel(\"Time (days)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_posterior_predictive_comparison(\n",
    "    [samples_npe, samples_pfn, samples_tsnpepfn],\n",
    "    [\"Standard NPE\", \"NPE-PFN\", \"TSNPE-PFN\"],\n",
    "    theta_o, ts_observed, time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7b2e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Methods Comparison\n",
    "\n",
    "| Method | Training | Simulations | Best For |\n",
    "|--------|----------|-------------|----------|\n",
    "| **Standard NPE** | Required (minutes) | Many (1000+) | Production, complex posteriors |\n",
    "| **NPE-PFN** | None! | Few (100) | Quick prototyping, iteration |\n",
    "| **TSNPE-PFN** | None! | Few (100) | Single observation, refinement |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Foundation models skip training**: NPE-PFN gives posteriors instantly using in-context learning\n",
    "2. **Trade-offs exist**: Foundation models may be less flexible than trained models for complex problems\n",
    "3. **Sequential variants help**: TSNPE-PFN focuses simulations for better efficiency\n",
    "4. **Great for prototyping**: Try NPE-PFN first, then train a custom model if needed\n",
    "\n",
    "\n",
    "**Further reading**: [NPE-PFN paper](https://arxiv.org/abs/2504.17660) | [GitHub repository](https://github.com/mackelab/npe-pfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cca3f",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Goals\n",
    "\n",
    "After this notebook, you should be able to:\n",
    "\n",
    "- ✅ Explain what foundation models are and why they're useful for SBI\n",
    "- ✅ Use NPE-PFN for instant posterior estimation without training\n",
    "- ✅ Apply TSNPE-PFN for sequential, observation-focused inference\n",
    "- ✅ Compare foundation models with standard NPE approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
