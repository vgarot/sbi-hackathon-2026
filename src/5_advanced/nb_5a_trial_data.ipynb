{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trial-Based Data and Mixed Parameter Types in SBI\n",
        "\n",
        "**Time: ~15 minutes**\n",
        "\n",
        "In many scientific applications, we observe **multiple data points per experiment** that are assumed to be independent and identically distributed (IID) given the same underlying parameters. For example:\n",
        "- Decision-making experiments with repeated trials\n",
        "- Neuroscience recordings across multiple sessions\n",
        "- Repeated measurements in physics experiments\n",
        "\n",
        "## What We'll Learn\n",
        "\n",
        "1. **NLE for trial-based data**: Train on single trials, infer from multiple trials\n",
        "2. **NPE with permutation-invariant embeddings**: Fully amortized inference over varying numbers of trials\n",
        "3. **MNPE for mixed parameter types**: Handle continuous AND discrete parameters\n",
        "\n",
        "**Key insight**: Different SBI methods handle trial-based data differently — NLE/NRE exploit IID structure automatically, while NPE requires special embedding networks!\n",
        "\n",
        "**Documentation**: [sbi IID tutorial](https://sbi.readthedocs.io/en/latest/advanced_tutorials/12_iid_data_and_permutation_invariant_embeddings.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from sbi.inference import NLE, NPE, MNPE\n",
        "from sbi.analysis import pairplot\n",
        "from sbi.neural_nets import posterior_nn\n",
        "from sbi.neural_nets.embedding_nets import FCEmbedding, PermutationInvariantEmbedding\n",
        "from sbi.inference.posteriors.posterior_parameters import MCMCPosteriorParameters\n",
        "\n",
        "from simulators import (\n",
        "    linear_gaussian_simulator,\n",
        "    create_trial_data_prior,\n",
        "    simulate_iid_trials,\n",
        "    true_posterior_linear_gaussian,\n",
        "    create_mixed_prior,\n",
        "    mixed_simulator,\n",
        ")\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup: A Simple Trial-Based Simulator\n",
        "\n",
        "We use a **Linear Gaussian** model — the simplest possible simulator for learning:\n",
        "\n",
        "$$\\theta \\sim \\text{Uniform}(-2, 2)^2$$\n",
        "$$x_i | \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2 I) \\quad \\text{for } i = 1, \\ldots, N$$\n",
        "\n",
        "Each trial $x_i$ is a noisy observation of the true parameter $\\theta$. With more trials, we get more information about $\\theta$, and the posterior becomes more concentrated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "dim = 2  # 2D parameter space\n",
        "noise_std = 0.5  # Observation noise\n",
        "prior = create_trial_data_prior(dim=dim)\n",
        "\n",
        "# True parameters for our \"observed\" data\n",
        "theta_o = torch.tensor([0.5, -0.3])\n",
        "\n",
        "print(f\"Prior: Uniform over [-2, 2]^{dim}\")\n",
        "print(f\"True parameters: θ = {theta_o.numpy()}\")\n",
        "print(f\"Observation noise: σ = {noise_std}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize how the posterior concentrates with more trials\n",
        "num_trials_list = [1, 5, 10, 20]\n",
        "\n",
        "# Generate observed data with different numbers of trials\n",
        "x_o_dict = {}\n",
        "for nt in num_trials_list:\n",
        "    x_o_dict[nt] = simulate_iid_trials(theta_o, num_trials=nt, noise_std=noise_std)\n",
        "\n",
        "# Get analytical posterior samples for visualization\n",
        "true_samples_dict = {}\n",
        "for nt in num_trials_list:\n",
        "    posterior = true_posterior_linear_gaussian(x_o_dict[nt], noise_std=noise_std)\n",
        "    true_samples_dict[nt] = posterior.sample((1000,))\n",
        "\n",
        "# Plot the analytical posteriors\n",
        "fig, ax = pairplot(\n",
        "    [true_samples_dict[nt] for nt in num_trials_list],\n",
        "    points=theta_o.unsqueeze(0),\n",
        "    diag=\"kde\",\n",
        "    upper=\"contour\",\n",
        "    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\"],\n",
        "    limits=[[-1.5, 2], [-2, 1.5]],\n",
        "    upper_kwargs=dict(levels=[0.95]),\n",
        "    fig_kwargs=dict(\n",
        "        points_colors=[\"k\"],\n",
        "        points_offdiag=dict(marker=\"*\", markersize=10),\n",
        "    ),\n",
        "    figsize=(6, 6),\n",
        ")\n",
        "plt.sca(ax[1, 1])\n",
        "plt.legend(\n",
        "    [f\"{nt} trial{'s' if nt > 1 else ''}\" for nt in num_trials_list] + [r\"$\\theta_o$\"],\n",
        "    frameon=False,\n",
        "    fontsize=10,\n",
        ")\n",
        "plt.suptitle(\"Analytical Posterior: More Trials -> Tighter Posterior\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"With more IID trials, the posterior concentrates around the true parameter!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Think First!\n",
        "\n",
        "Before we implement inference, let's think about how different SBI methods handle IID data:\n",
        "\n",
        "**Question 1**: In standard NPE, what is the input to the neural network? Why is this problematic for IID data?\n",
        "\n",
        "**Question 2**: NLE estimates $p(x|\\theta)$. How can we use this for inference with multiple IID observations?\n",
        "\n",
        "**Question 3**: What property must an embedding network have to handle IID data with varying numbers of trials?\n",
        "\n",
        "<details>\n",
        "<summary>Click to reveal answers</summary>\n",
        "\n",
        "1. **NPE input problem:**\n",
        "   - NPE takes observation $x$ as input and outputs posterior parameters\n",
        "   - If $x$ is a set of IID trials $\\{x_1, ..., x_N\\}$, the network needs:\n",
        "     - Fixed input size (but N varies!)\n",
        "     - Invariance to the order of trials (IID means order doesn't matter!)\n",
        "   - Standard NPE can't handle this directly!\n",
        "\n",
        "2. **NLE with IID data:**\n",
        "   - Train NLE on **single trials** to learn $p(x|\\theta)$\n",
        "   - For IID data: $p(\\{x_i\\}|\\theta) = \\prod_i p(x_i|\\theta)$ (or sum log-likelihoods)\n",
        "   - The joint likelihood can then be used with MCMC/VI for inference\n",
        "   - No retraining needed for different numbers of trials!\n",
        "\n",
        "3. **Permutation invariance:**\n",
        "   - The embedding must be **permutation invariant**: $f(\\{x_1, x_2, x_3\\}) = f(\\{x_3, x_1, x_2\\})$\n",
        "   - Achieved by: embedding each trial separately, then aggregating (sum/mean)\n",
        "   - Must also handle **variable-size** inputs for amortization over N\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: NLE for Trial-Based Data\n",
        "\n",
        "**Neural Likelihood Estimation (NLE)** is naturally suited for IID data:\n",
        "\n",
        "1. Train on **single-trial** data: Learn $p(x|\\theta)$ for individual observations\n",
        "2. At inference: Combine likelihoods across trials: $p(\\{x_i\\}|\\theta) = \\prod_i p(x_i|\\theta)$\n",
        "3. Use MCMC to sample from the posterior\n",
        "\n",
        "**Key advantage**: Train once, infer with any number of trials!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate single-trial training data for NLE\n",
        "num_simulations = 1000\n",
        "\n",
        "theta_train = prior.sample((num_simulations,))\n",
        "x_train = linear_gaussian_simulator(theta_train, noise_std=noise_std)\n",
        "\n",
        "print(f\"Training data: {num_simulations} single-trial simulations\")\n",
        "print(f\"theta shape: {theta_train.shape}, x shape: {x_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Task: Train NLE and perform inference with multiple trials\n",
        "\n",
        "Complete the code below to:\n",
        "1. Create and train an NLE model on single-trial data\n",
        "2. Build a posterior and sample given 10 observed trials\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MCMC parameters for NLE inference\n",
        "mcmc_parameters = MCMCPosteriorParameters(\n",
        "    method=\"slice_np_vectorized\",\n",
        "    num_chains=20,\n",
        "    thin=5,\n",
        "    warmup_steps=50,\n",
        "    init_strategy=\"proposal\",\n",
        ")\n",
        "\n",
        "# TODO: Create NLE inference object\n",
        "# nle = ...\n",
        "\n",
        "# TODO: Train NLE on single-trial data\n",
        "# nle...\n",
        "\n",
        "# TODO: Build posterior with MCMC\n",
        "# posterior_nle = ...\n",
        "\n",
        "print(\"NLE trained on single-trial data!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now sample from the posterior given 10 IID trials\n",
        "num_trials_test = 10\n",
        "x_o_10 = x_o_dict[10]  # 10 trials\n",
        "\n",
        "print(f\"Observed data shape: {x_o_10.shape} (10 trials, 2D each)\")\n",
        "\n",
        "# TODO: Sample from the posterior (sbi automatically handles IID data!)\n",
        "# samples_nle = ...\n",
        "\n",
        "print(f\"Posterior samples shape: {samples_nle.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare NLE posterior with analytical posterior\n",
        "true_posterior_10 = true_posterior_linear_gaussian(x_o_10, noise_std=noise_std)\n",
        "samples_true_10 = true_posterior_10.sample((2000,))\n",
        "\n",
        "fig, ax = pairplot(\n",
        "    [samples_nle, samples_true_10],\n",
        "    points=theta_o.unsqueeze(0),\n",
        "    diag=\"kde\",\n",
        "    upper=\"contour\",\n",
        "    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\"],\n",
        "    limits=[[-0.5, 1.5], [-1.5, 0.5]],\n",
        "    figsize=(6, 6),\n",
        ")\n",
        "plt.sca(ax[1, 1])\n",
        "plt.legend([\"NLE\", \"Analytical\", r\"$\\theta_o$\"], frameon=False)\n",
        "plt.suptitle(\"Part 1: NLE with 10 trials\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"NLE successfully handles multiple IID trials without retraining!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: NPE with Permutation-Invariant Embedding\n",
        "\n",
        "For **fully amortized** inference over varying numbers of trials, we can use NPE with a **permutation-invariant embedding network**.\n",
        "\n",
        "The idea:\n",
        "1. **Single-trial embedding**: A network that embeds each trial $x_i$ into a latent space\n",
        "2. **Aggregation**: Combine embeddings via a permutation-invariant operation (e.g., mean/sum)\n",
        "\n",
        "**Trade-off**: NPE requires training on data with varying numbers of trials, but inference is instant (no MCMC)!\n",
        "\n",
        "### Training data construction\n",
        "\n",
        "To train NPE to be amortized over the number of trials, we need to:\n",
        "1. For each parameter set, simulate data with **varying** numbers of trials\n",
        "2. Pad missing trials with NaN (so all training examples have the same shape)\n",
        "3. Use `z_score_x=\"none\"` because NaN values interfere with z-scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate training data with varying numbers of trials\n",
        "max_num_trials = 20\n",
        "num_training_samples = 1000\n",
        "\n",
        "# Sample base parameters\n",
        "theta_base = prior.sample((num_training_samples,))\n",
        "\n",
        "# Create training data: for each parameter set, create examples with 1, 2, ..., max_num_trials trials\n",
        "# Unobserved trials are masked with NaN\n",
        "x_npe = torch.ones(num_training_samples * max_num_trials, max_num_trials, dim) * float(\"nan\")\n",
        "\n",
        "for i in range(num_training_samples):\n",
        "    # Simulate max_num_trials trials for this parameter\n",
        "    xi = linear_gaussian_simulator(theta_base[i].repeat(max_num_trials, 1), noise_std=noise_std)\n",
        "    # Create training examples with varying numbers of trials\n",
        "    for j in range(max_num_trials):\n",
        "        x_npe[i * max_num_trials + j, :j + 1, :] = xi[:j + 1, :]\n",
        "\n",
        "# Repeat theta to match\n",
        "theta_npe = theta_base.repeat_interleave(max_num_trials, dim=0)\n",
        "\n",
        "print(f\"Training data shapes:\")\n",
        "print(f\"  theta: {theta_npe.shape}\")\n",
        "print(f\"  x: {x_npe.shape} (batch, max_trials, dim)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Task: Build and train NPE with permutation-invariant embedding\n",
        "\n",
        "Complete the code below to:\n",
        "1. Create a `PermutationInvariantEmbedding` network\n",
        "2. Build a density estimator with this embedding\n",
        "3. Train NPE and sample from the posterior\n",
        "\n",
        "**The key components**:\n",
        "1. Single-trial embedding network\n",
        "2. Permutation-invariant wrapper\n",
        "3. Create density estimator with embedding\n",
        "\n",
        "\n",
        "**Note**: We use `z_score_x=\"none\"` because NaN padding interferes with z-scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the permutation-invariant embedding network\n",
        "latent_dim = 10\n",
        "\n",
        "# Create single-trial embedding network\n",
        "single_trial_net = FCEmbedding(\n",
        "    input_dim=dim,  # Each trial is 2D\n",
        "    num_hiddens=40,\n",
        "    num_layers=2,\n",
        "    output_dim=latent_dim,\n",
        ")\n",
        "\n",
        "# TODO: Wrap in permutation-invariant embedding\n",
        "# embedding_net = PermutationInvariantEmbedding...\n",
        "\n",
        "# TODO: Create density estimator with embedding\n",
        "# density_estimator = posterior_nn...\n",
        "\n",
        "print(\"Permutation-invariant embedding network created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train NPE with the permutation-invariant embedding\n",
        "\n",
        "# TODO: Create NPE with the custom density estimator\n",
        "# npe = NPE...\n",
        "\n",
        "# TODO: Train (exclude_invalid_x=False because we use NaN padding)\n",
        "# npe...\n",
        "\n",
        "# TODO: Build posterior\n",
        "# posterior_npe = ...\n",
        "\n",
        "print(\"NPE with permutation-invariant embedding trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample from the posterior given 10 IID trials\n",
        "# We need to pad the observation to match the training format\n",
        "x_o_padded = torch.ones(1, max_num_trials, dim) * float(\"nan\")\n",
        "x_o_padded[0, :num_trials_test, :] = x_o_10\n",
        "\n",
        "# SOLUTION\n",
        "# TODO: Sample from NPE posterior\n",
        "samples_npe_perm = posterior_npe.sample((2000,), x=x_o_padded)\n",
        "\n",
        "print(f\"Posterior samples shape: {samples_npe_perm.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all methods\n",
        "fig, ax = pairplot(\n",
        "    [samples_nle, samples_npe_perm, samples_true_10],\n",
        "    points=theta_o.unsqueeze(0),\n",
        "    diag=\"kde\",\n",
        "    upper=\"contour\",\n",
        "    labels=[r\"$\\theta_1$\", r\"$\\theta_2$\"],\n",
        "    limits=[[-0.5, 1.5], [-1.5, 0.5]],\n",
        "    figsize=(6, 6),\n",
        ")\n",
        "plt.sca(ax[1, 1])\n",
        "plt.legend([\"NLE\", \"NPE (perm-inv)\", \"Analytical\", r\"$\\theta_o$\"], frameon=False, fontsize=9)\n",
        "plt.suptitle(\"Part 2: NPE with Perm-Inv Embedding\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"NPE is fully amortized - instant inference for any number of trials (up to max_num_trials)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: MNPE for Mixed Parameter Types\n",
        "\n",
        "What if your model has both **continuous** AND **discrete** parameters?\n",
        "\n",
        "Examples:\n",
        "- Model selection (discrete) + model parameters (continuous)\n",
        "- Binary switches + tuning parameters\n",
        "- Categorical choices + real-valued settings\n",
        "\n",
        "**MNPE (Mixed Neural Posterior Estimation)** extends NPE to handle this!\n",
        "\n",
        "### The Mixed Simulator\n",
        "\n",
        "We use a simple simulator with:\n",
        "- $\\theta_1 \\in \\mathbb{R}$: Continuous parameter (Gaussian prior)\n",
        "- $\\theta_2, \\theta_3 \\in \\{0, 1\\}$: Binary discrete parameters (Bernoulli prior)\n",
        "\n",
        "The observations are:\n",
        "- $x_1 = \\theta_1 + \\epsilon_1$ where $\\epsilon_1 \\sim \\mathcal{N}(0, 0.1^2)$\n",
        "- $x_2 = \\theta_2 + \\theta_3 + \\epsilon_2$ where $\\epsilon_2 \\sim \\mathcal{N}(0, 0.01^2)$\n",
        "\n",
        "**Key**: $x_2$ depends on the **sum** of discrete parameters, so $(0,1)$ and $(1,0)$ produce similar observations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for MNPE\n",
        "dim_continuous = 1\n",
        "dim_discrete = 2\n",
        "\n",
        "# Create mixed prior (continuous must come first for MNPE!)\n",
        "mixed_prior = create_mixed_prior(dim_continuous=dim_continuous, dim_discrete=dim_discrete)\n",
        "\n",
        "# True parameters for observation\n",
        "theta_o_mixed = torch.tensor([[0.5, 0.0, 1.0]])  # continuous=0.5, discrete=(0,1)\n",
        "x_o_mixed = mixed_simulator(theta_o_mixed)\n",
        "\n",
        "print(f\"True parameters: θ = {theta_o_mixed[0].numpy()}\")\n",
        "print(f\"  - Continuous (θ₁): {theta_o_mixed[0, 0].item():.2f}\")\n",
        "print(f\"  - Discrete (θ₂, θ₃): ({int(theta_o_mixed[0, 1].item())}, {int(theta_o_mixed[0, 2].item())})\")\n",
        "print(f\"Observation: x = {x_o_mixed[0].numpy()}\")\n",
        "print(f\"  - x₁ ≈ θ₁ = {x_o_mixed[0, 0].item():.3f}\")\n",
        "print(f\"  - x₂ ≈ θ₂ + θ₃ = {x_o_mixed[0, 1].item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Task: Train MNPE and perform inference\n",
        "\n",
        "Complete the code below to:\n",
        "1. Generate training data with the mixed simulator\n",
        "2. Create and train an MNPE model\n",
        "3. Sample from the posterior and compare with analytical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate training data for MNPE\n",
        "num_simulations_mnpe = 2000\n",
        "\n",
        "# TODO: Sample from mixed prior\n",
        "# theta_mnpe = ...\n",
        "\n",
        "# TODO: Simulate observations\n",
        "# x_mnpe = ...\n",
        "\n",
        "print(f\"Training data shapes: theta={theta_mnpe.shape}, x={x_mnpe.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train MNPE\n",
        "\n",
        "# TODO: Create MNPE density estimator\n",
        "# density_estimator_mnpe = ...\n",
        "\n",
        "# TODO: Create MNPE inference object\n",
        "# mnpe = ...\n",
        "\n",
        "# TODO: Train MNPE\n",
        "# mnpe...\n",
        "\n",
        "# TODO: Build posterior\n",
        "# posterior_mnpe = ...\n",
        "\n",
        "print(\"MNPE trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample from MNPE posterior\n",
        "num_samples = 10_000\n",
        "\n",
        "# SOLUTION\n",
        "# TODO: Sample from MNPE posterior\n",
        "samples_mnpe = posterior_mnpe.sample((num_samples,), x=x_o_mixed, show_progress_bars=False)\n",
        "\n",
        "print(f\"MNPE samples shape: {samples_mnpe.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare MNPE with analytical posterior\n",
        "# Following the toy.ipynb style: separate pairplots\n",
        "\n",
        "# MNPE posterior\n",
        "_ = pairplot(\n",
        "    samples_mnpe,\n",
        "    points=theta_o_mixed,\n",
        "    limits=[[-2, 2]] * dim_continuous + [[-0.1, 1.1]] * dim_discrete,\n",
        "    labels=[r\"$\\theta_1$ (cont)\"] + [f\"$\\theta_{i+2}$ (disc)\" for i in range(dim_discrete)],\n",
        "    figsize=(5, 5),\n",
        ")\n",
        "plt.suptitle(\"Samples from MNPE posterior\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Methods for Trial-Based Data\n",
        "\n",
        "| Method | Training Data | Inference | Pros | Cons |\n",
        "|--------|--------------|-----------|------|------|\n",
        "| **NLE** | Single trials | MCMC | Simple training, flexible | Requires MCMC per observation |\n",
        "| **NPE + Perm-Inv** | Varying #trials | Forward pass | Fully amortized | More complex training data |\n",
        "\n",
        "### MNPE for Mixed Parameters\n",
        "\n",
        "| Feature | Standard NPE | MNPE |\n",
        "|---------|-------------|------|\n",
        "| Continuous parameters | ✓ | ✓ |\n",
        "| Discrete parameters | ✗ | ✓ |\n",
        "| Mixed types | ✗ | ✓ |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **NLE is naturally suited for IID data**: Train on single trials, multiply likelihoods at inference\n",
        "2. **NPE needs special handling**: Use `PermutationInvariantEmbedding` for amortization over trials\n",
        "3. **MNPE extends NPE to mixed types**: Handles both continuous and discrete parameters\n",
        "4. **Trade-offs exist**: NLE is simpler to train but requires MCMC; NPE with embeddings is fully amortized but needs more complex training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Learning Goals\n",
        "\n",
        "After this notebook, you should be able to:\n",
        "\n",
        "- Explain how NLE exploits the IID structure of trial-based data\n",
        "- Use `PermutationInvariantEmbedding` for NPE with varying numbers of trials\n",
        "- Apply MNPE for inference with mixed continuous and discrete parameters\n",
        "- Choose the appropriate SBI method for different data structures"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
